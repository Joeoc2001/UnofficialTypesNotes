\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{txfonts}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{stmaryrd}
\usepackage{color}
\usepackage{bussproofs}
\usepackage{multicol}
\usepackage{setspace}
\graphicspath{ {./} }

\author{J. O'Connor}
\date{December 2021}

\setlength\parindent{0pt}
\setlength{\parskip}{1em}
\setlength{\columnsep}{0.5cm}

\lstset{
    language=ML,
    basicstyle=\small\sffamily,
    frame=single,
    tabsize=4,
    columns=fixed,
    showstringspaces=false,
    showtabs=false,
    keepspaces,
    commentstyle=\color{red},
    keywordstyle=\color{blue},
    xleftmargin=.05\textwidth,
    xrightmargin=.05\textwidth
}

\newcommand{\centerbox}[1]{
    \begin{center}
    \fbox{
        \begin{minipage}{\dimexpr\textwidth-2.5cm}
        #1
        \end{minipage}
    }
    \end{center}
}

\newcommand{\centerboxtitled}[2]{
    \centerbox {
        \begin{center}
            \textbf{#1}
        \end{center}
        
        #2
    }
}

\newcommand{\SafeRightLabel}[1]{
    \RightLabel{$\textrm{#1}$}
}

\newcommand{\SequentAxiom}[2]{
    \SafeRightLabel{#1}
    \SequentAxiomNoLabel{#2}
}

\newcommand{\SequentAxiomNoLabel}[1]{
    \AxiomC{}
    \UnaryInfC{#1}
    \DisplayProof
    \hspace{10px}
}

\newcommand{\SequentUnary}[3]{
    \SafeRightLabel{#1}
    \SequentUnaryNoLabel{#2}{#3}
}

\newcommand{\SequentUnaryNoLabel}[2]{
    \AxiomC{#1}
    \UnaryInfC{#2}
    \DisplayProof
    \hspace{10px}
}

\newcommand{\SequentBinary}[4]{
    \SafeRightLabel{#1}
    \SequentBinaryNoLabel{#2}{#3}{#4}
}

\newcommand{\SequentBinaryNoLabel}[3]{
    \AxiomC{#1}
    \AxiomC{#2}
    \BinaryInfC{#3}
    \DisplayProof
    \hspace{10px}
}

\newcommand{\SequentTrinary}[5]{
    \SafeRightLabel{#1}
    \SequentTrinaryNoLabel{#2}{#3}{#4}{#5}
}

\newcommand{\SequentTrinaryNoLabel}[4]{
    \AxiomC{#1}
    \AxiomC{#2}
    \AxiomC{#3}
    \TrinaryInfC{#4}
    \DisplayProof
    \hspace{10px}
}

\newcommand{\SequentBox}[1]{
    \centerbox{
        \vspace{15px}
        \begin{center}
        \begin{spacing}{3.0}
            #1
            \hspace{-10px}
        \end{spacing}
        \end{center}
        \vspace{-30px}
    }
}

\newcommand{\inlineeq}[1]{
    \vspace{-2em}
    \begin{gather*}
    #1
    \end{gather*}
    \vspace{-2em}
}

\newcommand{\quasi}[0]{\; \sim \!}

\setcounter{secnumdepth}{0}

\begin{document}

\begin{titlepage}
    \begin{center}
        \vspace*{1cm}
            
        \Huge
        \textbf{Unofficial Types Notes}
            
        \vspace{0.5cm}
        \LARGE
        Student written notes for the type theory course, based on the 2021 lectures
            
        \vspace{1.5cm}
            
        \textbf{J. O'Connor} 
        
        jo429
            
        \vfill
            
        THIS IS NOT AN OFFICIAL DOCUMENT! This was written by students to collate and reinforce their own understanding and should not be used as a substitute for the official documents. There may be missing content, things may be written incorrectly or misunderstood and entire points may be missed. 
            
        \vspace{0.8cm}
        
        \Large
        Thanks to the following people for proofreading this document: 
        
        R. Laine
        
        K. Druciarek
            
    \end{center}
\end{titlepage}

\setlength{\parskip}{-0.5em}
\tableofcontents
\setlength{\parskip}{1em}

\newpage
\section{Types}

Adding types to a programming language's design seems like an obvious choice\footnote{except to Lisp programmers} to make; we can add some guarantees to the format and uses of data, which can be checked at compile time via Type Checking, allowing for Type Safety to be guaranteed before the program is even run. Type systems are often also used as a form of documentation, where in the names of types may dictate the function of the data stored within instances of the types. This leads naturally to Object Oriented Programming, where the types not only represent the format and guarantees on the data stored within the type, but also the functions that are allowed to be called on the data. Type systems also lead to faster code, as the compilers for type safe languages can make strong assumptions about the data stored in a memory location based on the type of the variable that the memory location corresponds to. 

However, type systems lead a double life. Once we begin to formalise some of the above notions of 'Type Safety' and 'Typing judgements', we begin to see a direct parallel between \textbf{Types}, and \textbf{Logic \& Proof}. 

\section{Semantics of Programming Languages Revision}

In IB Semantics of Programming Languages we saw how we can define a grammar of a language, then an Operational Semantics and Typing Judgement for terms within this grammar. In II Types we start with an understanding of these concepts.

For example, we may have a simple language of Booleans and Integers. Its grammar may look like the following:

\inlineeq{
e ::= true \; | \; false \; | \; n \; | \; e_1 \leq e_2 \; | \; e_1 + e_2 \; | \; e_1 \land e_2 \; | \; \lnot e
}

From this grammar, we can begin to build terms:

\inlineeq{
3 + 4 \leq 5 \\
(3 + 4 \leq 7) \land (7 \leq 3 + 4) 
}

Excellent! However, we can also build some terms that don't make sense:

\inlineeq{
4 \land true \\
false + 7
}

The obvious thing to do is to modify our grammar to only allow for valid terms. In this language, we can do this by splitting the terms into three rules:

\inlineeq{
e_1 ::= n \; | \; e_1 + e_2 \\
e_2 ::= true \; | \; false \; | \; e_1 \leq e_1 \; | \; e_2 \land e_2 \; | \; \lnot e_2 \\
e ::= e_1 \; | \; e_2
}

By doing this, we have actually introduced a basic notion of types. We can see that $e_1$ is all of the expressions with type \textit{number} and $e_2$ is all of the expressions with type \textit{boolean}.

Unfortunately, then, we have managed to entangle our concepts of types and expressions. To disentangle these ideas, we introduce \textit{typing judgements} on expressions, where a judgement only exists if an expression is typed correctly, or \textit{well-typed}. This means that we can have 

Returning to our original language of Booleans and Integers:

\inlineeq{
e ::= true \; | \; false \; | \; n \; | \; e_1 \leq e_2 \; | \; e_1 + e_2 \; | \; e_1 \land e_2 \; | \; \lnot e
}

We can add judgement rules for every expression with a type as follows:

\SequentBox{
    \SequentAxiom{Num}{$n : \mathbb{N}$}
    \SequentAxiom{True}{$true : bool$}
    \SequentAxiom{False}{$false : bool$}
    \SequentUnary{Neg}{$e : bool$}{$\lnot e : bool$}
    \SequentBinary{Plus}{$ e : \mathbb{N} $}{$ e' : \mathbb{N} $}{$e + e' : \mathbb{N}$}
    \SequentBinary{And}{$ e : bool $}{$ e' : bool $}{$e \land e' : bool$}
    \SequentBinary{LEQ}{$ e : \mathbb{N} $}{$ e' : \mathbb{N} $}{$e \leq e' : bool$}
}

Note that we have not yet defined how these expressions actually behave when we step through their execution. Typing is a process done on expressions before they are run, and does not change the path that the expression takes when it is executed. Typing judgements simply tell us whether an expression is \textit{well-typed}, and we can show that well-typed expressions within some languages have nice properties like \textbf{Termination}.

\newpage
But now comes the question of variables. For example, the following statement should only be valid if the variable $x$ stores a number:

\inlineeq{
(x + x) \leq 10
}

To do this we add a context which holds the type of every variable in the expression. This context propagates through the proof tree as we build it from the bottom up, allowing us to see the types of variables within the local context in which they occupy.

\SequentBox{
    $\textrm{Contexts} \; \Gamma ::= \cdot \; | \; \Gamma, x: \tau$

    \SequentAxiom{Num}{$\Gamma \vdash n : \mathbb{N}$}
    \SequentAxiom{True}{$\Gamma \vdash true : bool$}
    \SequentAxiom{False}{$\Gamma \vdash false : bool$}
    \SequentBinary{Plus}{$\Gamma \vdash e : \mathbb{N} $}{$\Gamma \vdash e' : \mathbb{N} $}{$\Gamma \vdash e + e' : \mathbb{N}$}
    \SequentBinary{And}{$\Gamma \vdash e : bool $}{$\Gamma \vdash e' : bool $}{$\Gamma \vdash e \land e' : bool$}
    \SequentBinary{LEQ}{$\Gamma \vdash e : \mathbb{N} $}{$\Gamma \vdash e' : \mathbb{N} $}{$\Gamma \vdash e \leq e' : bool$}
    \SequentUnary{Var}{$x:\tau \in \Gamma$}{$\Gamma \vdash x : \tau$}
    \SequentBinary{Let}{$\Gamma \vdash e : \tau $}{$\Gamma, x:\tau \vdash e' : \tau' $}{$\Gamma \vdash \textrm{let} \; x = e \; \textrm{in} \; e' : \tau '$}
}

\newpage
\section{Structural Properties and Substitution}

We have introduced variables into our language, so we should introduce a notion of substitution as well:

\inlineeq{
\begin{split}
[e/x]true &= true \\
[e/x]false &= false \\
[e/x]n &= n \\
[e/x](e_1 + e_2) &= [e/x]e_1 + [e/x]e_2 \\
[e/x](e_1 \leq e_2) &= [e/x]e_1 \leq [e/x]e_2 \\
[e/x](e_1 \land e_2) &= [e/x]e_1 \land [e/x]e_2 \\
[e/x]x &= e \\
[e/x]z &= z \\
[e/x](\textrm{let} \; z = e_1 \; \textrm{in }e_2) &= \textrm{let }z = [e/x]e_1 \; \textrm{in} \; [e/x]e_2 \\
(\textrm{assuming} \; z &\notin dom(e))
\end{split}
}

These rules are akin to $\beta$-reduction in Lambda Calculus, and should be rules that you are very familiar with. Note that we did not need to define these substitution rules when defining the sequents for types in the previous section - we only use substitution when evaluating an expression, or when describing properties that hold for well-typed expressions.

There are three properties that we like to have hold for any type system. These are as follows:

\begin{enumerate}
\item (\textbf{Weakening})

If a term typechecks in a context, then it will still typecheck in a bigger context.

$\Gamma, \Gamma' \vdash e : \tau \implies \Gamma, x : \tau'', \Gamma' \vdash e : \tau$

\item (\textbf{Exchange})

If a term typechecks in a context, then it will still typecheck after reordering the variables in the context.

$\Gamma, x_1 : \tau_1, x_2 : \tau_2, \Gamma' \vdash e : \tau \implies
\Gamma, x_2 : \tau_2, x_1 : \tau_1, \Gamma' \vdash e : \tau$

\item (\textbf{Substitution})

Subsituting a type-correct term for a variable will preserve type correctness.

$(\Gamma \vdash e : \tau) \land (\Gamma, x : \tau \vdash e' : \tau') \implies
\Gamma \vdash [e/x]e' : \tau'$
\end{enumerate}

These properties are all proven by structural induction in the lectures. That is, by proving that each property holds for each possible structure of expression independently, we prove that all expressions must have these properties.

\newpage
\section{Operational Semantics}

We have a language and type system. We therefore have an idea for what programs are valid within our language and also what the type of the value calculated will be. How do we say what value a program computes? With an \textit{Operational Semantics}, a two-place relation on terms $e \rightsquigarrow e'$, pronounced as "e steps to e prime".


\SequentBox{
    $\textrm{Values} \; v ::= n \; | \; true \; | \; false$
    
    \SequentUnary{AndCong}{$e_1 \rightsquigarrow e_1'$}{$e_1 \land e_2 \rightsquigarrow e_1' \land e_2$}
    \SequentAxiom{AndTrue}{$true \land e \rightsquigarrow e$}
    \SequentAxiom{AndFalse}{$false \land e \rightsquigarrow false$}
    \SequentUnary{LEQCong1}{$e_1 \rightsquigarrow e_1'$}{$e_1 \leq e_2 \rightsquigarrow e_1' \leq e_2$}
    \SequentUnary{LEQCong2}{$e \rightsquigarrow e'$}{$v \leq e \rightsquigarrow v \leq e'$}
    \SequentUnary{LEQTrue}{$n_1 \leq n_2$}{$n_1 \leq n_2 \rightsquigarrow true$}
    \SequentUnary{LEQFalse}{$n_1 > n_2$}{$n_1 \leq n_2 \rightsquigarrow false$}
    \SequentUnary{AddCong1}{$e_1 \rightsquigarrow e_1'$}{$e_1 + e_2 \rightsquigarrow e_1' + e_2$}
    \SequentUnary{AddCong2}{$e \rightsquigarrow e'$}{$v + e \rightsquigarrow v + e'$}
    \SequentUnary{AddStep}{$n_1 + n_2 = n_3$}{$n_1 + n_2 \rightsquigarrow n_3$}
    \SequentUnary{LetCong}{$e_1 \rightsquigarrow e_1'$}{$\textrm{let} \; z = e_1 \; \textrm{in} \; e_2 \rightsquigarrow \textrm{let} \; z = e_1' \; \textrm{in} \; e_2$}
    \SequentAxiom{LetStep}{$\textrm{let} \; z = v \; \textrm{in} \; e \rightsquigarrow [v / z] e$}
}

A reduction sequence is a sequence of transitions $e_1  \rightsquigarrow e_2, e_2 \rightsquigarrow e_3, ..., e_{n-1} \rightsquigarrow e_n \implies e_1 \rightsquigarrow^* e_n$
A term $e$ is stuck if it is not a value, and there is no $e'$ such that $e \rightsquigarrow e'$

\newpage
Stuck terms are erroneous programs with no defined behaviour, so we like to avoid them. Therefore we have two properties that we like to have hold:

\begin{enumerate}
\item  (\textbf{Progress})

Well-typed programs are not stuck: they can always take a step of progress (or are done).

$\cdot  \vdash e : \tau \implies (e \in v) \lor (\exists e'. e \rightsquigarrow e')$

\item  (\textbf{Preservation})

If a well-typed program takes a step, it will stay well-typed.

$(\cdot  \vdash e : \tau) \land (e \rightsquigarrow e') \implies \cdot  \vdash e' : \tau$
\end{enumerate}

All five of these key properties of \textbf{Weakening}, \textbf{Exchange}, \textbf{Substitution}, \textbf{Progress} and \textbf{Preservation} are collectively known as \textit{Type Safety}

These properties are also proven by structural induction in the lectures.

\newpage
\section{Typed Lambda Calculus}

The above language is fine, but it has a lot of fluff with numbers and addition and so on. We've seen that we can represent computability with Lambda Calculus, which doesn't come with any fancy numbers or booleans. We can add types to lambda calculus exactly as you'd expect: 

\begin{equation*}
\begin{split}
X ::= 1 &| X \times Y \; | \; 0 \; | \; X + Y \; | \; X \to Y \\
e ::= x &| \langle \rangle  \; | \; \langle e, e'\rangle  \; | \; \textrm{fst} \; e \; | \; \textrm{snd} \; e \; | \; \textrm{abort} \; e \\
&| L e \; | \; R e \; | \; \textrm{case}(e, L x \to e', R y \to e'') \\
&| \lambda x : X. e \; | \; e e' \\
\Gamma ::= \cdot  &| \Gamma, x : X
\end{split}
\end{equation*}

Where the only 'base' types are unit $\langle \rangle $  with type $1$ and the empty type $\bot$ that cannot be constructed with type $0$. See the next section to understand why the empty type is in our system.

Note that we have inherently restricted the expressions that we can form with this new typed calculus. This was actually our aim - we want to prevent the formation of expressions that do not have type safety. However our notation of type safety at this point is restrictive, and there are some expressions within lambda calculus which terminate but cannot be expressed here. For example, you cannot create the Ackermann function within STLC, but we could within untyped Lambda Calculus.

Also of interest is that we've added product and sum types to our simply typed lambda calculus, which weren't present in the original lambda calculus. This is because in LC we can represent these ideas purely through functions and function applications, however our type system is restrictive and prevents us from encoding these structures (See Polymorphic Lambda Calculus later on for more detail). Therefore we add product and sum expressions, just to preserve some of the computability of Lambda Calculus. Other typed lambda caluli we can add other combinations of these expressions, but our STLC has function applications, products, sums and the empty type $\bot$.

\newpage
The typing derivations are as follows:

\SequentBox{
    \SequentAxiom{1I}{$\Gamma\vdash\langle\rangle : 1$}
    \SequentBinary{$\times$I}{$\Gamma\vdash e : X$}{$\Gamma\vdash e' : Y$}{$\Gamma\vdash \langle e,e' \rangle : X \times Y$}
    \SequentUnary{$\times $E$_1$}{$\Gamma\vdash e : X \times Y$}{$\Gamma\vdash \textrm{fst} \; e : X$}
    \SequentUnary{$\times $E$_2$}{$\Gamma\vdash e : X \times Y$}{$\Gamma\vdash \textrm{snd} \; e : Y$}
    \SequentUnary{HYP}{$x : X \in \Gamma$}{$\Gamma\vdash x : X$}
    \SequentUnary{$\to$I}{$\Gamma, x:X \vdash e:Y$}{$\Gamma \vdash \lambda x : X . e : X \to Y$}
    \SequentBinary{$\to$E}{$\Gamma \vdash e : X \to Y$}{$\Gamma \vdash e' : X$}{$\Gamma \vdash e e' : Y$}
    \SequentUnary{$+$I$_1$}{$\Gamma \vdash e : X$}{$\Gamma \vdash L e : X + Y$}
    \SequentUnary{$+$I$_2$}{$\Gamma \vdash e : Y$}{$\Gamma \vdash R e : X + Y$}
    \SequentTrinary{$+$E}{$\Gamma \vdash e : X + Y$}{$\Gamma, x: X \vdash e' : Z$}{$\Gamma, y: Y \vdash e'' : Z$}{$\Gamma \vdash \textrm{case}(e, Lx \to e', Ry \to e'') : Z$}
    \SequentUnary{0E}{$\Gamma \vdash e : 0$}{$\Gamma \vdash \textrm{abort} \; e : Z$}
}

Note that we cannot build any expression of type 0, but if we could then we could abort it to create any other type we want. We will see that this is akin to being able to prove anything from falsehood.

\newpage
We then add the operational semantics:

\SequentBox{
   $\textrm{Values }v ::= \langle \rangle  \; | \; \langle v, v'\rangle  \; | \; \lambda x : A. e \; | \; L v \; | \; R v$
   
   \SequentUnaryNoLabel{$e_1 \rightsquigarrow e_1'$}{$\langle e_1, e_2 \rangle \rightsquigarrow \langle e_1', e_2 \rangle$}
   \SequentUnaryNoLabel{$e_2 \rightsquigarrow e_2'$}{$\langle v_1, e_2 \rangle \rightsquigarrow \langle v_1, e_2' \rangle$}
   
   \SequentAxiomNoLabel{$\textrm{fst }\langle v_1, v_2 \rangle \rightsquigarrow v_1$}
   \SequentAxiomNoLabel{$\textrm{snd }\langle v_1, v_2 \rangle \rightsquigarrow v_2$}
   
   \SequentUnaryNoLabel{$e \rightsquigarrow e'$}{$\textrm{fst} \; e \rightsquigarrow \textrm{fst} \; e'$}
   \SequentUnaryNoLabel{$e \rightsquigarrow e'$}{$\textrm{snd} \; e \rightsquigarrow \textrm{snd} \; e'$}
   
   \SequentUnaryNoLabel{$e \rightsquigarrow e'$}{$\textrm{abort} \; e \rightsquigarrow \textrm{abort} \; e'$}
   
   \SequentUnaryNoLabel{$e \rightsquigarrow e'$}{$Le \rightsquigarrow Le'$}
   \SequentUnaryNoLabel{$e \rightsquigarrow e'$}{$Re \rightsquigarrow Re'$}
   
   \SequentUnaryNoLabel{$e \rightsquigarrow e'$}{$\textrm{case}(e, Lx \to e_1, Ry \to e_2) \rightsquigarrow \textrm{case}(e', Lx \to e_1, Ry \to e_2)$}
   
   \SequentAxiomNoLabel{$\textrm{case}(Lv, Lx \to e_1, Ry \to e_2) \rightsquigarrow [v / x]e_1$}
   \SequentAxiomNoLabel{$\textrm{case}(Rv, Lx \to e_1, Ry \to e_2) \rightsquigarrow [v / y]e_2$}
   
   \SequentUnaryNoLabel{$e_1 \rightsquigarrow e_1'$}{$e_1 e_2 \rightsquigarrow e_1' e_2$}
   \SequentUnaryNoLabel{$e_2 \rightsquigarrow e_2'$}{$v_1 e_2 \rightsquigarrow v_1 e_2'$}
   
   \SequentAxiomNoLabel{$(\lambda x : X . e) v \rightsquigarrow [v / x] e$}
}

The five key properties of \textbf{Weakening}, \textbf{Exchange}, \textbf{Substitution}, \textbf{Progress} and \textbf{Preservation} all hold for these rules.

\newpage
\section{The Curry-Howard Correspondence}

Imagine that we have a program that has type $A \to B$. Then we know that if we have an instance of the type $A$ then we can feed it in to the program and get an instance of the type $B$.

This looks suspiciously like Modus Ponens. That is, if we assume that having an instance of a type is equivalent to proving that the type is 'true', then we have derived the following rule:

\begin{center}
    \SequentBinary{}{A}{A $\to$ B}{B}
\end{center}

This shows a mapping between a function of type $A\to B$ and implication $A' \implies B'$ where $A'$ and $B'$ are the prepositions that the types $A$ and $B$ correspond to.

We can also see that, if having an instance of a type is equivalent to proving that the corresponding preposition is 'true', then the type that corresponds to falsehood must be the type that has no instance. We call this type $\bot$

Continuing in this line of reasoning, we can imagine Conjunction as product types since we must have evidence for both elements to generate the product, and Disjunction as Unions since we only need evidence of a single of the elements to generate the union.

It is also useful to establish the concept of a normal form for a proof. For example, take the following expression:

\inlineeq{
\lambda x:A.\lambda y:A\to B.yx
}

This expression has the type $A \to (A \to B) \to B$

We could also consider a slightly more complicated function:

\inlineeq{
\lambda x:A.\lambda y:A\to B.((\lambda c:A\to B.c)y)((\lambda b:A.b)x)
}

This expression also has the type $A \to (A \to B) \to B$

Therefore both expressions are an instance of the type corresponding to Modus Ponens, so both functions can be interpreted as proofs for the same thing.

However if you study the expressions for a bit you'll notice that there is some fluff in the second expression that will be evaluated away immediately: $(\lambda b:A.b)x \rightsquigarrow [x/b]b = x$. In fact, the second expression will reduce down to the first.

You'll also notice that the first expression is a value. That is, it cannot be reduced any further. Therefore we can assert that the the first expression is a normal form, and proof normalisation is equivalent to stepping the expression to a value.

\newpage
This gives us the following table:

\begin{center}
\begin{tabular}{ c|c} \; 
 Logic & Programming \\
 \hline
 Formulas & Types \\
 Proofs & Programs \\
 Truth & Unit \\
 Falsehood & Empty Type \\
 Conjugation & Products \\
 Disjunction & Unions \\
 Implication & Functions \\
 Normal Form & Value \\
 Proof Normalization & Evaluation \\
 Normalization Strategy & Evaluation Order \\
\end{tabular}
\end{center}

We can construct an expression of Negation using our idea of Falsehood, of $\bot$. Since a type being false corresponds to there being no object that inhabits the type, then proving that a statement is false is equivalent to proving that if the statement were true, we would be able to inhabit $\bot$.

Therefore the logical statement $\lnot A$ equates to the type $A \to \bot$.

Interestingly, we find that we cannot prove some statements within this system that we would normally take as valid. For example, we cannot construct a function of the following type:

\inlineeq{((A \to \bot) \to \bot) \to A}

Which means that, within this system of proving statements by finding instances of types, we cannot prove that $\lnot \lnot A \implies A$ (Double Negation Elimination). We also find that we cannot prove $A \lor \lnot A$ (Law of Excluded Middle).

Overall, the set of logical statements that we are able to prove with the Curry-Howard Correspondence does not contain every logical statement that we can prove to be true in Classical Logic. This system of logic is instead called Intuitionistic Propositional Logic, and is in fact equivalent to Classical Logic, just without DNE as an axiom.

\newpage
\section{Not Halting and Falsehood}

We said before that there should be no way to generate an instance of $\bot$, as it should equate to falsehood and since any instance of a type equates to the type being true, then $\bot$ should have no instances. If we were able to generate a proof (i.e an example program) which suggests from its type that we can generate a term of type $\bot$ then our logic is inconsistent.

Since we know that there is no value of type $\bot$, and also that all well-typed programs in STLC must be a value or progress, we know that the only hope that we have to create a term that types to $\bot$ would be a term that loops forever and does not halt.

However if we try to create a function which loops forever (and so types to $\bot$) we find that we cannot type it. For example the infinite looping function $\Omega$:

\inlineeq{
\Omega = (\lambda x. x x)(\lambda x. x x)
}

We know that $\Omega$ loops forever, but what would its type be? Well if we try to start figuring it out, we can see that for each half of $\Omega$, the parameter $x$ must have a type. This type must be a function, since $x$ is applied to something, and the parameter of the function must be the type of $x$. So we get that $x: A\to B$ and also that $A=A \to B$. This infinite type is not allowed within STLC so $\Omega$ is not a well-typed expression.

This is valid intuition as to why STLC cannot loop forever (and so we cannot build expressions that have type $\bot$), but for a proof we need to be more rigorous.

\newpage
\section{A Proof of Termination}

So far in this course we have used structural induction to prove most of the properties that we want to hold. Unfortunately, as seen in the slides, proving termination using structural induction is not possible. This is because the result of function application may itself be a function application, and so we cannot use the inductive hypothesis to assume that the result of function application terminates since we are in the process of proving if function application terminates!

Instead we first build a collection of sets of terms named \textit{Halt}, where the following hold:

\begin{itemize}
    \item $Halt_0 = \emptyset $ 
    
    i.e. for all $e$, $e \notin Halt_0$
    
    \item $e \in Halt_1$  when $e$ halts
    
    \item $e \in Halt_{X \to Y}$ when:
    
    \begin{itemize}
        \item $e \in Halt_1$  (i.e. $e$ halts)
        \item $\forall e' \in Halt_X. (e e') \in Halt_Y$
    \end{itemize}
\end{itemize}

We can therefore read a set \textit{Halt} in the following way:

\begin{itemize}
    \item $Halt_1$ is the set of expressions that halt.
    \item $Halt_{1 \to 1}$ is the set of expressions that halt when applied to an expression that also halts.
    \item $Halt_{(1 \to 1) \to 1}$ is the set of expressions that halt when applied to an expression $e'$ that halts when applied to an expression that halts.
    \item $Halt_{1 \to (1 \to 1)}$ is the set of expressions that, when applied to an expression that halts, result in an expression which preserves halting.
    \item $Halt_{(1 \to 1) \to (1 \to 1)}$ is the set of expressions that, when applied to an expression which preserves halting, result in an expression which preserves halting.
\end{itemize}

and so on.

So essentially for all expressions in a language to always halt, all expressions $e$ in that language must be in $Halt_1$, though this is not a property that we will attempt to prove, since we only really care whether $\bot$ can be formed, which can be proven in a different way using the fundamental lemma.

\newpage
\section{Closure Lemma}

Before we prove the \textit{fundamental lemma} we first prove the \textit{closure lemma}. That is:

\inlineeq{
e \rightsquigarrow e' \implies (e' \in Halt_X \iff e \in Halt_X)
}

Or, in english, if some property of halting or preserving halting is held by some expression $e$ then any expression that steps to $e$ and any expression that $e$ steps to must also preserve that property of halting or preserving halting.

We prove the statement by induction on $X$ and this can be seen in the lecture slides.

\section{Fundamental Lemma}

The fundamental lemma is as follows:

\inlineeq{
x_1 : X_1, . . ., x_n : X_n \vdash e : Z\\
\land \space \space \forall i \in  {1 . . . n}. (\cdot \vdash v_i : X_i) \land (v_i \in  Halt_{X_i}) \\
\implies [v_1/x_1, . . ., v_n/x_n]e \in  Halt_Z
}

That is, if we have an expression $e$ which has type $Z$ and free variables $x_1$ to $x_n$ with types $X_1$ to $X_n$, and we can show that there are values $v_1$ to $v_n$ with types $X_1$ to $X_n$ that all preserve halting as described by their respective type $X_i$, then if we substitute all of these values in to the variables in $e$ we get an expression that preserves halting as described by the type $Z$.

Or, in English, haltingness is invarient under the $\rightsquigarrow$ relation.

This is interesting because we get the halting property for $e$ seemingly from nowhere. The clever part is that when we apply an expression to a value, we add the value to the set of substitutions and so we can refer back to the inductive hypothesis (step 8 for $Case \to l$)

And from this we can prove \textbf{Consistency}:

\inlineeq{
\textrm{There are no terms }\cdot  \vdash e : 0
}

\begin{enumerate}
\item Assume $\cdot  \vdash e : 0$
\item $e \in  Halt_0$ by Fundamental lemma
\item $Halt_0 = \emptyset $ by definition
\end{enumerate}

which gives a contradiction.

\newpage
\section{The Halting Problem}

Since every closed program reduces to a value, and there are no values of empty type, there are no programs of the empty type. But the only programs of the empty type are the ones that do not halt. So have we avoided the halting problem?

\inlineeq{
e \; \textrm{well-formed} \land \cdot \vdash e : \tau \implies e \; \textrm{Halts}\\
e \in \textrm{Simply Typed Lambda Calculus} \implies e \; \textrm{Halts}
}

The thing to notice is that this isn't a bi-implication! There are programs within LC that do halt but are not accepted by STLC, e.g. \textit{ack}. So then how can we make STLC stronger?

\section{Loops}

We know from Foudations of Computer Science that while loops can be represented using unbounded recursion. However unfortunately adding unbounded recursion runs us straight into the issue that we were trying to avoid above; we can construct terms that loop forever and so typecheck to 0. For example:

\inlineeq{
\cdot \vdash (fun_{1\to 0} f x. f x) \langle \rangle : 0
}

However we do know that recursion with a base case that will be hit, or a for loop with a bounded upper bound, will always terminate as long as the code being run within that loop also terminates. This leads us to adding bounded recursion, and inventing Gödel’s T.

\newpage
\section{Gödel’s T}

Gödel’s T begins as STLC, but we add integers and bounded iteration over those integers. This gives us the following grammar:

\inlineeq{
\begin{split}
X ::= 1 \; &| \; X \times Y \; | \; 0 \; | \; X + Y \; | \; X \to Y \; | \; \mathbb{N} \\
e ::= x \; &| \; \langle \rangle  \; | \; \langle e, e'\rangle  \; | \; \textrm{fst} \; e \; | \; \textrm{snd} \; e \; | \; \textrm{abort} \; e \\
&| \; L e \; | \; R e \; | \; \textrm{case}(e, L x \to e', R y \to e'') \\
&| \; \lambda x : X. e \; | \; e e' \\ 
&| \; z \; | \; s(e) \; | \; \textrm{iter}(e, z \to e', s(x) \to e'') \\
\Gamma ::= \cdot  \; &| \; \Gamma, x : X
\end{split}
}

And the following rules \textbf{in addition to the ones within STLC}:

\SequentBox{
    \SequentAxiom{$\mathbb{N}I_z$}{$\Gamma \vdash z : \mathbb{N}$}
    \SequentUnary{$\mathbb{N}I_s$}{$\Gamma \vdash e : \mathbb{N}$}{$\Gamma \vdash s(e) : \mathbb{N}$}
    
    \SequentTrinary{$\mathbb{N}E$}{$\Gamma \vdash e_0 : \mathbb{N}$}{$\Gamma \vdash e_1 : X$}{$\Gamma, x:X \vdash e_2 : X$}{$\Gamma \vdash \textrm{iter}(e_0, z \to e_1, s(x) \to e_2) : X$}
    
    \SequentUnaryNoLabel{$e_0 \rightsquigarrow e_0'$}{$\textrm{iter}(e_0, z \to e_1, s(x) \to e_2) \rightsquigarrow \textrm{iter}(e_0', z \to e_1, s(x) \to e_2)$}
    \SequentAxiomNoLabel{$\textrm{iter}(z, z \to e_1, s(x) \to e_2) \rightsquigarrow e_1$}
    \SequentAxiomNoLabel{$\textrm{iter}(s(v), z \to e_1, s(x) \to e_2) \rightsquigarrow [\textrm{iter}(v, z \to e_1, s(x) \to e_2)/x]e_2$}
}

We can see that the above language is at least as powerful as primitive recursion, however it is a little bit more powerful than that. It can't be as powerful as partial recursion since:
\begin{enumerate}
    \item we have still preserved our property of every well-typed expression halting
    \item being as powerful as partial recursion would mean the ability to compute all computable functions
\end{enumerate} 
Therefore being as powerful as partial recursion would violate The Halting Problem. But it sits somewhere in-between. For example, \textit{ack} is computable with Gödel’s T but not by primitive recursion.

\newpage
\section{More Data Structures}

We have just added integers within Gödel’s T, but we might want to go further and add some more useful structures like lists.

The naive approach is to add these structures to our language through sequents:

\SequentBox{
    \SequentAxiom{ListNil}{$\Gamma \vdash [] : \textrm{list} \; X$}
    \SequentBinary{ListCons}{$\Gamma \vdash e : X$}{$\Gamma \vdash e' : \textrm{list} \; X$}{$\Gamma \vdash e :: e' : \textrm{list} \; X$}
    \SequentTrinary{ListFold}{$\Gamma \vdash e_0 : \textrm{list} \; X$}{$\Gamma \vdash e_1 : Z$}{$\Gamma, x:X, r:Z \vdash e_2:Z$}{$\Gamma \vdash \textrm{fold}(e_0, [] \to e_1, x::r \to e_2 ): Z$}
    \SequentUnaryNoLabel{$e_0 \rightsquigarrow e_0'$}{$e_0 :: e_1 \rightsquigarrow e_0' :: e_1$}
    \SequentUnaryNoLabel{$e_1 \rightsquigarrow e_1'$}{$v_0 :: e_1 \rightsquigarrow v_0 :: e_1'$}
    \SequentUnaryNoLabel{$e_0 \rightsquigarrow e_0'$}{$\textrm{fold}(e_0, [] \to e_1, x::r \to e_2 ) \rightsquigarrow \textrm{fold}(e_0', [] \to e_1, x::r \to e_2 )$}
    \SequentAxiomNoLabel{$\textrm{fold}([], [] \to e_1, x::r \to e_2 ) \rightsquigarrow e_1$}
    \SequentUnaryNoLabel{$R \triangleq \textrm{fold}(v', [] \to e_1, x::r \to e_2 )$}{$\textrm{fold}(v::v', [] \to e_1, x::r \to e_2 ) \rightsquigarrow [v/x, R/r] e_2$}
}

However the biggest issue here is that now when we define functions over these lists within this Typed Lambda Calculus we must define the type of the list within the function signature. For example, consider the map function:

\inlineeq{
\lambda f:A \to B. \lambda xs: \textrm{List} \; A.\\ \; \textrm{fold}(xs, [] \to [], x :: r \to (f x) :: r)
}

We must define the specific $A$ and $B$ that this map function will use at the time of definition, even though we can run the function with any different map $f$.

The solution is polymorphism.

\newpage
\section{Polymorphic Lambda Calculus (AKA System F)}

To add polymorphism, we need to extend our type representation to be able to represent type 'variables, or types that have not yet been filled in. At this stage, we only want to think about polymorphism that accepts all types, i.e. universal quantification over types. This is because universal quantification is the one that solves our polymorphic map issue as above; we don't care about the type of the variables in our list as long as our function to apply to all of the elements matches the list elements then the map function will work. Later we will see the concept of existential quantification, but it serves a different purpose.

\inlineeq{
\textrm{Types} \; A ::= \alpha  \; | \; A \to B \; | \; \forall \alpha . A
}

You might notice that we have removed both the unit and empty types. We will show later that we can represent data using polymorphics without base data types like unit.

We also need to extend our terms definition since we need to be able to now also abstract over types. We do this by a sort of lambda abstraction, just like we do in regular basic lambda calculus over variables. These lambda abstractions, written with a big lambda $\Lambda$, take a type rather than a value to substitute in.

\inlineeq{
\textrm{Terms} \; e ::= x \; | \; \lambda x : A. e \; | \; e e' \; | \; \Lambda \alpha . e \; | \; e A
}

This gives us some power to represent the map function that we wanted:

\inlineeq{
map : \forall \alpha . \forall \beta . (\alpha  \to \beta ) \to \textrm{list} \; \alpha  \to \textrm{list} \; \beta  \\
map = \Lambda \alpha.\Lambda \beta. \lambda f:\alpha \to \beta. \lambda xs: \textrm{List} \; \alpha.\\ \; \textrm{fold}(xs, [] \to [], x :: r \to (f x) :: r)
}

Well-formedness of types for these expressions is more tricky since we need to keep track of what type variables are currently being abstracted over - a type is not valid if it contains $\alpha$ while $\alpha$ is not currently abstracted over.

We introduce a type context $\Theta$ that holds all of the currently abstracted over variables. From this set we can deduce if a type is well-formed. We can think of this adding an extra step: it is no longer good enough to type an expression $e:T$, we must also ensure that $\Theta  \vdash T \; \textrm{type}$.

\newpage
The rules for checking $\Theta$ are as follows:

\SequentBox{
    \begin{spacing}{1.0}
        \inlineeq{
        \begin{split}
        \textrm{Type Contexts} \; \Theta &::= \cdot  \; | \; \Theta, \alpha \\
        \textrm{Term Contexts} \; \Gamma &::= \cdot  \; | \; \Gamma, x:A\\
        \end{split}
        }
    \end{spacing}
    
    \vspace{20px}
    
    \SequentUnaryNoLabel{$\alpha \in \Theta$}{$\Theta \vdash \alpha \; \textrm{type}$}
    \SequentBinaryNoLabel{$\Theta \vdash A \; \textrm{type}$}{$\Theta \vdash B \; \textrm{type}$}{$\Theta \vdash A \to B \; \textrm{type}$}
    \SequentUnaryNoLabel{$\Theta, \alpha \vdash A \; \textrm{type}$}{$\Theta \vdash \forall \alpha.A \; \textrm{type}$}
    \SequentUnaryNoLabel{$x:A \in \Gamma$}{$\Theta; \Gamma \vdash x : A$}
    \SequentBinaryNoLabel{$\Theta \vdash A \; \textrm{type}$}{$\Theta ; \Gamma, x : A \vdash e : B$}{$\Theta ; \Gamma \vdash \lambda x : A. e : A \to B$}
    \SequentBinaryNoLabel{$\Theta ; \Gamma \vdash e : A \to B$}{$\Theta ; \Gamma \vdash e' : A$}{$\Theta ; \Gamma \vdash e e' : B$}
    \SequentUnaryNoLabel{$\Theta, \alpha ; \Gamma \vdash e : B$}{$\Theta ; \Gamma \vdash \Lambda \alpha . e : \forall \alpha . B$}
    \SequentBinaryNoLabel{$\Theta ; \Gamma \vdash e : \forall \alpha . B$}{$\Theta \vdash A \; \textrm{type}$}{$\Theta ; \Gamma \vdash e A : [A / \alpha] B$}
}

Note the presence of substitution in the typing rules! In regular Lambda Calculus we substitute in the operational semantics when $\lambda$ terms are applied to values, whereas in PLC we substitute in the typing rules when the $\Lambda$ terms are applied to types.

For these rules, we'd like to ensure that \textbf{Weakening}, \textbf{Exchange} and \textbf{Substitution} apply to the type well-formedness $\Theta$ rules:

\begin{enumerate}
\item (\textbf{Type Weakening})

$\Theta, \Theta' \vdash A \; \textrm{type} \implies \Theta, \beta, \Theta' \vdash A \; \textrm{type}$

\item (\textbf{Type Exchange})

$\Theta, \beta, \gamma, \Theta' \vdash A \; \textrm{type} \implies \Theta, \gamma, \beta, \Theta' \vdash A \; \textrm{type}$

\item (\textbf{Type Substitution})

$(\Theta  \vdash A \; \textrm{type}) \land (\Theta, \alpha  \vdash B \; \textrm{type}) \implies 
\Theta  \vdash [A/\alpha ]B \; \textrm{type}$
\end{enumerate}

These all follow essentially the same format as \textbf{Weakening}, \textbf{Exchange} and \textbf{Substitution} for the $\Gamma$ well-typedness equivalents, both in the way they are presented above and the inductive proofs.

\newpage

We can lift these up a level and say that an entire context is well formed if each type in the context is well formed under $\Theta$:

\SequentBox{
    \SequentAxiomNoLabel{$\Theta \vdash \cdot \; \textrm{ctx}$}
    \SequentBinaryNoLabel{$\Theta \vdash \Gamma \; \textrm{ctx}$}{$\Theta \vdash \tau \; \textrm{type}$}{$\Theta \vdash \Gamma, \tau \; \textrm{ctx}$}
}

A well-formed context has the three key properties:

\begin{enumerate}
\item (\textbf{Context Weakening})

$\Theta, \Theta' \vdash \Gamma \; \textrm{ctx} \implies \Theta, \beta, \Theta' \vdash \Gamma \; \textrm{ctx}$

\item (\textbf{Context Exchange})

$\Theta, \beta, \gamma, \Theta' \vdash \Gamma \; \textrm{ctx} \implies \Theta, \gamma, \beta, \Theta' \vdash \Gamma \; \textrm{ctx}$

\item (\textbf{Context Substitution})

$(\Theta  \vdash A \; \textrm{type}) \land (\Theta, \alpha  \vdash \Gamma \; \textrm{ctx}) \implies 
\Theta  \vdash [A/\alpha ]\Gamma \; \textrm{ctx}$
\end{enumerate}

To prove these properties is trivial by induction on the size of $\Gamma$

We also have a property of \textbf{Regularity}:

\inlineeq{
(\Theta  \vdash \Gamma \; \textrm{ctx}) \land (\Theta ; \Gamma \vdash e : A) \implies \Theta  \vdash A \; \textrm{type}
}

In English, this just says if typechecking succeeds, and the context is well-formed, then we found a well-formed type.

We also still want to prove \textbf{Weakening}, \textbf{Exchange} and \textbf{Substitution} for terms, not just types and contexts. This gives us 6 more iterations, since we can modify $\Theta$ or $\Gamma$ within the Antecedent:

\begin{enumerate}
\item (\textbf{Type Weakening of Terms})

$(\Theta, \Theta' \vdash \Gamma \; \textrm{ctx}) \land (\Theta, \Theta '; \Gamma \vdash e : A) \implies \Theta, \alpha, \Theta '; \Gamma \vdash e : A$

\item (\textbf{Type Exchange of Terms})

$(\Theta, \alpha, \beta, \Theta' \vdash \Gamma \; \textrm{ctx}) \land
(\Theta, \alpha, \beta, \Theta '; \Gamma \vdash e : A) \implies \Theta, \beta, \alpha, \Theta '; \Gamma \vdash e : A$

\item (\textbf{Type Substitution of Terms})

$(\Theta, \alpha  \vdash \Gamma \; \textrm{ctx}) \land (\Theta  \vdash A \; \textrm{type})
\land (\Theta, \alpha ; \Gamma \vdash e : B) \implies \Theta ; [A/\alpha ]\Gamma \vdash [A/\alpha ]e : [A/\alpha ]B$

\item (\textbf{Weakening of Terms})

$(\Theta  \vdash \Gamma, \Gamma' \; \textrm{ctx}) \land (\Theta  \vdash B \; \textrm{type}) \land (\Theta ; \Gamma, \Gamma' \vdash e : A) \implies \Theta ; \Gamma, y: B, \Gamma' \vdash e : A$

\item (\textbf{Exchange of Terms})

$(\Theta  \vdash \Gamma, y : B, z : C, \Gamma' \; \textrm{ctx}) \land
(\Theta ; \Gamma, y : B, z : C, \Gamma' \vdash e : A) \implies \Theta ; \Gamma, z : C, y : B, \Gamma' \vdash e : A$

\item (\textbf{Substitution of Terms})

$(\Theta  \vdash \Gamma, x : A \; \textrm{ctx}) \land (\Theta ; \Gamma \vdash e : A)
\land (\Theta ; \Gamma, x : A \vdash e' : B) \implies \Theta ; \Gamma \vdash [e/x]e' : B$
\end{enumerate}

This brings us to a total of 4 variants of \textbf{Weakening}, \textbf{Exchange} and \textbf{Substitution}, plus a single \textbf{Regularity} rule. To prove some of the rules we have to assume well-formedness conditions, but the proofs are all otherwise similar to STLC.

And then the operational semantics:

\SequentBox{
    $\textrm{Values} \; v ::= \lambda x : A. e \; | \; \Lambda \alpha . e$
    
    \SequentUnary{CongFun}{$e_0 \rightsquigarrow e_0'$}{$e_0 e_1 \rightsquigarrow e_0' e_1$}
    \SequentUnary{CongFunArg}{$e_1 \rightsquigarrow e_1'$}{$v_0 e_1 \rightsquigarrow v_0 e_1'$}
    \SequentAxiom{FunEval}{$(\lambda x : X . e) v \rightsquigarrow [v / x] e$}
    
    \SequentUnary{CongForAll}{$e \rightsquigarrow e'$}{$e A \rightsquigarrow e' A$}
    \SequentAxiom{ForAllEval}{$(\Lambda \alpha . e) A \rightsquigarrow [A / \alpha] e$}
}

The first three sequents are ripped straight from STLC, and the last two are added just to deal with our new type lambdas. I find it interesting that we only need these five operational semantics rules, rather than the fifteen in STLC.

And as per usual we want the two type safety rules:

\begin{enumerate}
\item (\textbf{Progress})

$\cdot  ; \cdot  \vdash e : \tau \implies (e \in v) \lor (\exists e'. e \rightsquigarrow e')$

\item (\textbf{Preservation})

$(\cdot  ; \cdot  \vdash e : \tau) \land (e \rightsquigarrow e') \implies \cdot  ; \cdot  \vdash e' : \tau$
\end{enumerate}

But where did the data go?

\newpage
\section{Data and System F}

Discovered in 1941 by Alonzo Church, the idea follows the following observations:

\begin{enumerate}
    \item Data is used to make choices
    \item Based on the choice, you perform different results
    \item So we can encode data as functions which take different possible results, and return the right one
\end{enumerate}

For an example, take a boolean. The only reason that a boolean is useful is if we can run an if statement on it. There is no use being able to store a boolean if we can't print it out, run a branching statement or modify other data based on the boolean's value.

We can imagine then that a boolean and a function which either executes the first branch or the second branch are equivalent. That is, if the only way that we can use a boolean is to either do a first thing or a second thing, then we may as well just encode the boolean as doing either the first or the second thing.

This gives us a type for a boolean of $\forall \alpha . \alpha  \to \alpha  \to \alpha $. Or in English, if you give me two $\alpha$ values, the boolean will either give the first value or the second value.

We can then encode true as $\Lambda \alpha . \lambda x : \alpha . \lambda y : \alpha . x$, getting the first value, and false as $\Lambda \alpha . \lambda x : \alpha . \lambda y : \alpha . y$, getting the second value.

We also get the if statement for free: 

\inlineeq{
\textrm{if} \; e \; \textrm{then} \; e' \; \textrm{else} \; e'' : X \implies e X e' e''
}

Another way to see this is from a functional point of view. In functional languages we define new types as tagged unions. These tagged unions can then be acted on using a match statement. Since the only way that a tagged union can be interrogated is this match statement, we can combine the two and encode the tagged union \textit{as} the match statement, that takes a set of values and gives back the value that the tagged union contains.

\begin{lstlisting}[language=ML]
type bool = True \; | \; False;

let encode (b: bool) (t: 'a) (f: 'a) = 
	match b with
	| True -> t
	| False -> f;;

let true: 'a -> 'a -> 'a = encode True;;
let false: 'a -> 'a -> 'a = encode False;;
\end{lstlisting}

We see that \textit{true} and \textit{false} both have type $\forall \alpha . \alpha  \to \alpha  \to \alpha $ as expected.

When we have a tagged union that contains some data, we must make sure that the encoded data is managed. For example, imagine a tagged union that holds either X or Y

\begin{lstlisting}[language=ML]
type union = L of x \; | \; R of y;
\end{lstlisting}

Then we need to manage where these values go when passed to the match function. The answer is to enforce that the values passed to the match function are functions that take X or Y:

\begin{lstlisting}[language=ML]
let encode (u: union) (l: x -> 'a) (r: y -> 'a) = 
	match u with
	| L(v) -> l v
	| R(v) -> r v;;

let left: (x -> 'a) -> (y -> 'a) -> 'a = encode (L x1);;
let right: (x -> 'a) -> (y -> 'a) -> 'a = encode (R y1);;
\end{lstlisting}

And from these types we can read off our encodings of unions within PLC:

\inlineeq{
\begin{split}
X + Y &\implies \forall \alpha . (X \to \alpha ) \to (Y \to \alpha ) \to \alpha \\
L e &\implies \Lambda \alpha . \lambda f : X \to \alpha . \lambda g : Y \to \alpha . f e\\
R e &\implies \Lambda \alpha . \lambda f : X \to \alpha . \lambda g : Y \to \alpha . g e
\end{split}
}

The Case statement is encoded with a bit more fluff but follows the same idea:

\inlineeq{
\textrm{case}(e, L x \to e1, R y \to e2) : Z \implies e Z (\lambda x : X \to Z. e1) (\lambda y : Y \to Z. e2)
}

We can encode product types in the same way, except far easier since we don't actually have a choice in the types stored:

\begin{lstlisting}[language=ML]
type prod = Some of (x * y);
\end{lstlisting}

Which gives us the following:

\inlineeq{
\begin{split}
X \times Y &\implies \forall \alpha . (X \to Y \to \alpha ) \to \alpha \\
\langle e, e'\rangle  &\implies \Lambda \alpha . \lambda k : X \to Y \to \alpha . k e e'\\
\textrm{fst} \; e &\implies e X (\lambda x : X. \lambda y : Y. x)\\
\textrm{snd} \; e &\implies e Y (\lambda x : X. \lambda y : Y. y)
\end{split}
}

Where this becomes more complicated is when the data that we add to the tagged union is recursive. For example, imagine a representation of integers. Then we might have the following type:

\begin{lstlisting}[language=ML]
type int = Zero \; | \; Succ of int;
\end{lstlisting}

Then the match statement for the Succ branch must take the variable stored, meaning the Succ value must be a function that takes an int:

\begin{lstlisting}[language=ML]
let encode (b: int) (z: 'a) (s: int -> 'a) = 
	match b with
	| Zero -> z
	| Succ(i) -> s i;;
\end{lstlisting}

This doesn't really help us to encode int since we still need to know the encoding of an int within our definition for the type of s.

The trick is to leverage that recursion into the function as follows:

\begin{lstlisting}[language=ML]
let encode (b: int) (z: 'a) (s: 'a -> 'a) = 
	match b with
	| Zero -> z
	| Succ(i) -> s (encode i z s);;
\end{lstlisting}

This gives us the following encodings:

\inlineeq{
\begin{split}
N &\implies \forall \alpha . \alpha  \to (\alpha  \to \alpha ) \to \alpha \\
z &\implies \Lambda \alpha . \lambda z : \alpha . \lambda s : \alpha  \to \alpha . z\\
s(e) &\implies \Lambda \alpha . \lambda z : \alpha . \lambda s : \alpha  \to \alpha . s (e \alpha  z s)\\
\textrm{iter}(e, z \to e_z, s(x) \to e_s) : X &\implies e X e_z (\lambda x : X. e_s)
\end{split}
}

And we can also do lists recursively too:

\begin{lstlisting}[language=ML]
type list 'x = Nil \; | \; Cons of ('x * list);

let encode (l: list 'x) (n: 'a) (c: 'x -> 'a -> 'a) = 
	match l with
	| Nil -> n
	| Cons(v, l) -> c v (encode l n c);;
\end{lstlisting}

Which gives the following:

\inlineeq{
\begin{split}
list X &\implies \forall \alpha . \alpha  \to (X \to \alpha  \to \alpha ) \to \alpha \\
[] &\implies \Lambda \alpha . \lambda n : \alpha . \lambda c : X \to \alpha  \to \alpha . n\\
e :: e' &\implies \Lambda \alpha . \lambda n : \alpha . \lambda c : X \to \alpha  \to \alpha . c e (e' \alpha  n c)
\end{split}
}

And we can define the three main list functions on this representation

\inlineeq{
fold(e, [] \to e_n, x :: r \to e_c) : Z = e Z e_n (\lambda x : X. \lambda r : Z. e_c)\\
map(e, f): listZ = fold(e, [] \to [], x::r \to (f x) :: r): listZ \\
filter(e, f): listZ = fold(e, [] \to [], x::r \to if(f x, x :: r, r)): listZ \\
}

\newpage
\section{Existential types}

Within modern programming languages, we use polymorphism not only to allow methods to run on any number of types, as we saw in the list example above, but we also use polymorphism to define interfaces behind which we hide implementation. For example, in Java we might define an interface as follows:

\begin{lstlisting}[language=Java]
interface Bool {
	public Bool getTrue();
	public Bool getFalse();
  public <T> T eval(T ifTrue, T ifFalse);
}
\end{lstlisting}

We can then require our argument to be any implementation of this interface, but we do not care about the actual way that the boolean is stored. We might have some obvious implementation backed by a boolean:

\begin{lstlisting}[language=Java]
class BoolBool implements Bool {
	boolean value;

	private BoolBool(boolean value) {
		this.value = value;
	}

	public Bool getTrue() {
		return new BoolBool(true);
	}

	public Bool getFalse() {
		return new BoolBool(false);
	}
	
	public <T> T eval(T ifTrue, T ifFalse) {
		return value ? ifTrue : ifFalse;
	}
}
\end{lstlisting}

\newpage
Or an integer:

\begin{lstlisting}[language=Java]
class IntBool implements Bool {
	private int value;
	
	private IntBool(int value) {
		this.value = value;
	}

	public Bool getTrue() {
		return new IntBool(1);
	}

	public Bool getFalse() {
		return new IntBool(0);
	}

	public <T> T eval(T ifTrue, T ifFalse) {
		return value == 1 ? ifTrue : ifFalse;
	}
}
\end{lstlisting}

Or any number of other implementations. The only important addition to being able to query the value is to also be able to construct the value.

The way that we can capture this concept within our type system is existential types, written $\cdot ; \cdot  \vdash e : \exists \alpha . A$ to mean 'Expression $e$ requires some implementation $\alpha$ of interface $A$'

We add the grammar for these implementations as follows:

\SequentBox{
    \begin{spacing}{1.0}
    \inlineeq{
    \begin{split}
    \textrm{Types} \; A &::= . . . \; | \; \exists \alpha . A\\
    \textrm{Terms} \; e &::= . . . \; | \; \textrm{pack}_{\alpha .B}(A, e) \; | \; \textrm{let pack}(\alpha, x) = e \; \textrm{in} \; e'\\
    \textrm{Values} \; v &::= . . . \; | \; \textrm{pack}_{\alpha .B}(A, v)
    \end{split}
    }
    \end{spacing}
    
    \vspace{30px}
    
    \SequentTrinary{$\exists l$}{$\Theta, \alpha \vdash B \; \textrm{type}$}{$\Theta \vdash A \; \textrm{type}$}{$\Theta; \Gamma \vdash e : [A/\alpha]B$}{$\Theta ; \Gamma \vdash \textrm{pack}_{\alpha . B}(A, e) : \exists \alpha . B$}
    \SequentTrinary{$\exists$E}{$\Theta ; \Gamma \vdash e : \exists \alpha . A$}{$\Theta, \alpha ; \Gamma, x : A \vdash e' : C$}{$\Theta \vdash C \; \textrm{type}$}{$\Theta ; \Gamma \vdash \textrm{let pack}(\alpha, x) = e \; \textrm{in} \; e' : C$}
}

\newpage
The concept here is that:

\begin{itemize}
    \item We can 'pack' both a type $A$ and an expression $e$ together to form an expression of type $\exists \alpha.B$, where $\alpha$ can appear within $B$, as long as $e$ has type $[A/\alpha]B$
    \item We can use this packed value in an expression $e'$ if it has both a free type $\alpha$ and a free variable $x$ of type $B$ (note that in the rules above we result in a pack expression with type $\exists \alpha. B$ but then use a pack expression of type $\exists \alpha. A$).
\end{itemize}

When we use a packed value in an expression, we want to substitute both the type and the value in at once. This gives us the operational semantics:

\SequentBox{
    \SequentUnaryNoLabel{$e \rightsquigarrow e'$}{$\textrm{pack}_{\alpha.B}(A, e) \rightsquigarrow \textrm{pack}_{\alpha.B}(A, e')$}
    \SequentUnaryNoLabel{$e \rightsquigarrow e'$}{$\textrm{let pack}(\alpha, x) = e \; \textrm{in} \; t \rightsquigarrow \textrm{let pack}(\alpha, x) = e' \; \textrm{in} \; t$}
    \SequentAxiomNoLabel{$\textrm{let pack}(\alpha, x) = \textrm{pack}_{\alpha.B}(A, v) \; \textrm{in} \; e \rightsquigarrow [A/\alpha, v/x]e$}
}

To see how this works in practice, consider our boolean example. A boolean interface needs to provide a true instance, a false instance, and an if statement. 

If $\alpha$ encodes a boolean value, then an if statement would have the signature

\inlineeq{
\textrm{if}(a, b, c): \alpha \to \delta \to \delta \to \delta
}

Therefore the boolean existential type might look something like:

\inlineeq{
\exists \alpha. (\alpha \times \alpha \times (\forall \delta . \alpha \to \delta \to \delta \to \delta))
}

Remember that we encode a tuple in System F as $\forall \beta. (X \to Y \to \beta) \to \beta$, so we can expand the tuple:

\inlineeq{
\exists \alpha . \forall \beta. (\alpha \to \alpha \to (\forall \delta . \alpha \to \delta \to \delta \to \delta) \to \beta) \to \beta
}

This gives us the type that we want for the boolean pack, which we can validate by working from the pack.

\newpage
To build a specific pack implementation we need a type and a value. The type that we had for our booleans before was $\forall \alpha . \alpha  \to \alpha  \to \alpha $. We also have true as $\Lambda \alpha . \lambda x : \alpha . \lambda y : \alpha . x$ and false as $\Lambda \alpha . \lambda x : \alpha . \lambda y : \alpha . y$. This means that our implementation \textbf{value} would be:

\inlineeq{
(\Lambda \alpha . \lambda x : \alpha . \lambda y : \alpha . x, \\\Lambda \alpha . \lambda x : \alpha . \lambda y : \alpha . y, \\ \Lambda A. \lambda b: \forall \alpha . \alpha  \to \alpha  \to \alpha . \lambda x: A. \lambda y: A. b x y) 
}

AKA a true value, a false value, and an if statement. Using the tuple encoding we get a System F value:
\begin{equation*}
\begin{split}
&\Lambda B.\lambda c: (\forall \alpha . \alpha  \to \alpha  \to \alpha )\to \\
&\quad(\forall \alpha . \alpha  \to \alpha  \to \alpha )\to \\
&\quad(\forall \delta. (\forall \alpha . \alpha  \to \alpha  \to \alpha ) \to \delta \to \delta \to \delta)\\
&\quad\to B. \\
&\qquad c (\Lambda \alpha . \lambda x : \alpha . \lambda y : \alpha . x) \\
&\qquad(\Lambda \alpha . \lambda x : \alpha . \lambda y : \alpha . y) \\
&\qquad(\Lambda A. \lambda b: \forall \alpha . \alpha  \to \alpha  \to \alpha . \lambda x: A. \lambda y: A. b x y)
\end{split}
\end{equation*}

Unpleasant but it's all there. If you want to try to read this and understand where it all comes in, imagine that c is a selector function that returns either its first argument (a value representing \textbf{True}), its second argument (a value representing \textbf{False}) or its third argument (a function representing \textbf{if}) If we type this value we get the following:

\inlineeq{
\forall \beta. ((\forall \alpha . \alpha  \to \alpha  \to \alpha )\to (\forall \alpha . \alpha  \to \alpha  \to \alpha )\to \\(\forall \delta . (\forall \alpha . \alpha  \to \alpha  \to \alpha )\to \delta \to \delta \to \delta) \to \beta) \to \beta
}

Which has a lot of $\forall \alpha . \alpha  \to \alpha  \to \alpha $ in it. Luckily this is also the type of our boolean implementation, and the idea of the pack expression is that we hide our specific implementation type and value. So we observe that this type is the same as:

\inlineeq{
[\forall \alpha . \alpha  \to \alpha  \to \alpha /\gamma ](\forall \beta. (\gamma \to \gamma \to \\(\forall \delta . \gamma \to \delta \to \delta \to \delta) \to \beta) \to \beta)
}

And voilà we have arrived at our boolean pack type again.

It is worth noting that we haven't actually extended the computational power of System F since we can encode existential types using type polymorphism:
\begin{equation*}
\begin{split}
\exists \alpha . B &\implies \forall \beta . (\forall \alpha. B \to \beta) \to \beta\\
\textrm{pack}_{\alpha.B}(A, e) &\implies \Lambda \beta . \lambda k : \forall \alpha . B \to \beta . k A e\\
\textrm{let pack}(\alpha, x) = e \; \textrm{in} \; e' : C &\implies e C (\Lambda \alpha / \lambda X : B . e')
\end{split}
\end{equation*}

\newpage
\section{System F Termination}

We already know that structural and rule induction were non-starters when it came to proving termination within STLC, and we can assume that the same applies here. Instead, System F can be proven to terminate using similar methods to when we proved termination for STLC. 

We define a \textit{Semantic Type} to be a similar concept to the \textit{Halt} sets when we were proving termination for STLC - A \textit{Semantic Type $X$} is a set of terms such that:

\begin{itemize}
    \item (\textbf{Halting})
    
    If some expression is in the semantic type then it halts
    
    $e \in  X \implies \exists v.e \rightsquigarrow^* v$
    
    \item (\textbf{Closure})
    
    If some expression in the semantic type steps to or is stepped to by another expression, that other expression is also in the semantic type
    
    $e \rightsquigarrow e' \implies (e \in X \iff e' \in X)$
\end{itemize}

The reason that we can't use the same \textit{Halt} sets as STLC is that not all types are valid, so we can't have all \textit{Halt} sets existing. The solution is to include $\Theta$ and only make Semantic Types over well-formed types.

From the two conditions above, just like our definition of Halt, we can see that each Semantic Type X with at least one expression has an associated type T. This can be seen by the type safety of stepping within the second point - if an expression with type T is in the Semantic Type then the normal form of that expression must also be in the Semantic Type, which means that all expressions that reduce to that normal form must be in the semantic type. We can't use this property to assume that all expressions of a type are in a Semantic Type since we are currently proving that System F actually terminates, so we haven't yet proven that all expressions of a type have a normal form.

Recall that we use $\Theta$ to represent a type variable context. That is a set of type variables $\alpha, \beta, ...$.

We can define $\theta$ to be a specific substitution of the generic types to specific semantic types, i.e.

\inlineeq{\theta ::= \cdot \; | \; (\theta, X/\alpha)}

\newpage
Then we define a function $\llbracket - \rrbracket$ such that:

\inlineeq{\llbracket - \rrbracket \in WellFormedType \to VarInterpretation \to SemanticType}

Since $\theta$ is a mapping from well formed types to semantics types, we can define the following:

\inlineeq{\llbracket \Theta \vdash \alpha \; \textrm{Type} \rrbracket = \theta (\alpha)}

We then know that all other valid types follow one of two other forms, either $A \to B$ or $\forall \alpha . B$

The $A \to B$ case is very similar to Halt; $e$ is in $\llbracket \Theta \vdash A \to B \; \textrm{Type} \rrbracket \theta$ if:

\begin{itemize}
    \item $e$ halts
    \item For all $e'$ in $\llbracket \Theta \vdash A \; \textrm{Type} \rrbracket \theta$ we have that $(e e')$ is in $\llbracket \Theta \vdash B \; \textrm{Type} \rrbracket \theta$
\end{itemize}

That is, $e$ preserves halting of expressions when applied to an expression. This too is similar to the interpretation of \textit{Halt}.

The new type form of System F is $\forall \alpha . B$; $e$ is in $\llbracket \Theta \vdash \forall \alpha . B \; \textrm{Type} \rrbracket \theta$ if:

\begin{itemize}
    \item $e$ halts
    \item For all Types $A$ and Semantic Types $X$ we have that $(e A)$ is in
    
    $\llbracket \Theta, \alpha \vdash B \; \textrm{Type} \rrbracket (\theta, X/\alpha )$
\end{itemize}

This can be interpreted as $e$ halting no matter what type it is instantiated with. Note that the type A and semantic type X don't have to be linked. The reason that we can do this is that technically type applications are only used for book keeping - they never modify the behaviour of a program. This is similar logic to type erasure in Java in that once we have proven that an expression is well-typed, the types can be stripped away and the program can be run without any of the type information. Therefore we can allow for any type A without checking that the semantic type X corresponds to the same type since the type A that we choose will not modify whether or not the program halts, only the semantic type X could possibly modify the halting behaviour of the program.

\newpage
We have a few properties that we can prove on these Semantic Types:

\begin{itemize}
    \item (\textbf{Closure})
    
    If $\theta$ is an interpretation for $\Theta$, then $\llbracket \Theta \vdash A \; \textrm{type} \rrbracket \theta$ is a semantic type.
    
    \item (\textbf{Exchange})
    
    $\llbracket \Theta, \alpha, \beta, \Theta' \vdash A \; \textrm{type}\rrbracket = \llbracket \Theta, \beta, \alpha, \Theta' \vdash A \; \textrm{type}\rrbracket$
    
    \item (\textbf{Weakening})
    
    If $\Theta \vdash A \; \textrm{type}$, then $\llbracket \Theta, \alpha \vdash A \; \textrm{type}\rrbracket (\theta, X/\alpha) = \llbracket \Theta \vdash A \; \textrm{type}\rrbracket \theta$
    
    \item (\textbf{Substitution})
    
    If $\Theta \vdash A \; \textrm{type}$ and $\Theta, \alpha \vdash B \; \textrm{type}$ then 
    
    $\llbracket \Theta \vdash [A/\alpha ]B \; \textrm{type}\rrbracket \theta = \llbracket \Theta, \alpha \vdash B \; \textrm{type}\rrbracket (\theta, \llbracket \Theta \vdash A \; \textrm{type}\rrbracket \theta)$
\end{itemize}

These properties can be proven by induction on the three structures of well formed types.

Which leads us to the fundamental lemma: If we have that
\begin{itemize}
    \item $\Theta = \alpha_1, ..., \alpha_k$
    \item $\Gamma = x_1: A_1, ..., x_n: A_n$
    \item $\Theta \vdash \Gamma$ ctx
    \item $\Theta; \Gamma \vdash e : B$
    \item $\theta$ interprets $\Theta$
    \item For all $x_i: A_i$ in $\Gamma$ we have some $e_i \in \llbracket \Theta \vdash A_i$ type $\rrbracket \theta$
\end{itemize}

Then we can substitute all free type variables with arbitrary types and all free variables with the expressions and gain the properties given by the semantic type set:

$$[C_1/\alpha_1, ..., C_k/\alpha_k][e_1/x_1, ..., e_n/x_n]e\in \llbracket \Theta \vdash B \; \textrm{type} \rrbracket \theta$$ 

And so, since all values within System F halt by definition, and all expressions can be built up by substitution of values into values, all expressions must be in their respective Semantic Type. And since all expressions in a Semantic Type halt, all expressions in System F must halt.

\newpage
\section{Second Order Intuitionistic Propositional Logic}

We have just seen that we can introduce both Universal and Existential quantifiers to our type system. This would imply that we can show that Second Order Intuitionistic propositions are true by finding instances of higher order types.

For example, imagine that we have propositions $P(x)$ and $Q(x)$ with corresponding types $\alpha \vdash T_P$ type and $\alpha \vdash T_Q$ type. Note the free type variables $\alpha$ modelling the parameter $x$. Then we may want to prove the following second order statement:

\inlineeq{\exists x. P(x) \land \forall x. (P(x) \implies Q(x)) \implies \exists x. Q(x)}

This has the following type:

\inlineeq{\exists \alpha. T_P \to (\forall \alpha. T_P \to T_Q) \to \exists \alpha. T_Q}

We can build the following term in System F with the above type:
\begin{equation*}
\begin{split}
&\lambda a : \exists \alpha. T_P. (\\
&\quad\lambda b. \forall \alpha. T_P \to T_Q. (\\
&\qquad\textrm{let pack}(\alpha, x) = a \; \textrm{in pack}_{\alpha.T_Q}(\alpha, b \alpha x)\\
&\quad)\\
&)
\end{split}
\end{equation*}

We will discuss what this means further when we introduce Classical Logic.

\newpage
\section{State and Stores}

Sometimes it'd be useful to actually harvest output from our code. The most intuitive way to do this is with a store and state, as if modelling a file system or memory block.

Borrowing from IB Semantics we can imagine a language with references to state locations that we can update. We might want to create a fresh reference (akin to malloc) using an instruction like \textbf{ref e}, read a reference with \textbf{!e} and update a reference with \textbf{e := e'}.

\centerboxtitled{Small Aside:}{
    Something not mentioned on the slides is that we will also need a sequence operator, as we will want to run instructions in sequence while we modify the state. However our operational semantics state that function application only occurs once both the function and the argument are values, so we can encode sequence in the following way:
    
    $$e_1 ; e_2 \implies (\lambda x. ) e_2) e_1$$
    
    Here $e_1$ must be fully evaluated before being substituted, and then $e_2$ will be fully evaluated and returned.
}

We first define our language grammar:

\begin{equation*}
\begin{split}
\textrm{Types X} &::= 1 \; | \; N \; | \; X \to Y \; | \; \textrm{ref} \; X\\
\textrm{Terms e} &::= \langle \rangle \; | \; n \; | \; \lambda x : X. e \; | \; e e' \; | \; \textrm{new} \; e \; | \; !e \; | \; e := e' \; | \; l\\
\textrm{Stores} \; \sigma &::= \cdot \; | \; \sigma, l : v\\
\textrm{Contexts} \; \Gamma &::= \cdot \; | \; \Gamma, x : X
\end{split}
\end{equation*}

\newpage
We then get the following operational semantics. Note that they are mostly just copied and pasted from STLC, but we keep a state $\sigma$ along for the ride. The only exceptions are the instructions which modify the state:


\SequentBox{
    $\textrm{Values v} ::= \langle \rangle \; | \; n |\lambda x : X. e \; | \; l$
    
    \SequentUnaryNoLabel{$\langle \sigma; e_0 \rangle \rightsquigarrow \langle \sigma'; e_0' \rangle$}{$\langle \sigma; e_0 e_1 \rangle \rightsquigarrow \langle \sigma'; e_0' e_1 \rangle$}
    \SequentUnaryNoLabel{$\langle \sigma; e_1 \rangle \rightsquigarrow \langle \sigma'; e_1' \rangle$}{$\langle \sigma; v_0 e_1 \rangle \rightsquigarrow \langle \sigma'; v_0 e_1' \rangle$}
    \SequentAxiomNoLabel{$\langle \sigma; (\lambda x : X . e) v \rangle \rightsquigarrow \langle \sigma; [v / x] e \rangle$}
    
    \SequentUnaryNoLabel{$\langle \sigma; e \rangle \rightsquigarrow \langle \sigma'; e' \rangle$}{$\langle \sigma; \textrm{new} \; e \rangle \rightsquigarrow \langle \sigma'; \textrm{new} \; e' \rangle$}
    \SequentUnaryNoLabel{$\langle \sigma; e \rangle \rightsquigarrow \langle \sigma'; e' \rangle$}{$\langle \sigma; ! e \rangle \rightsquigarrow \langle \sigma'; ! e' \rangle$}
    
    \SequentUnaryNoLabel{$l \notin dom(\sigma)$}{$\langle \sigma; \textrm{new} \; v \rangle \rightsquigarrow \langle (\sigma, l : v); l \rangle$}
    \SequentUnaryNoLabel{$l:v \in \sigma$}{$\langle \sigma; !l \rangle \rightsquigarrow \langle \sigma; v \rangle$}
    
    \SequentUnaryNoLabel{$\langle \sigma; e_0 \rangle \rightsquigarrow \langle \sigma'; e_0' \rangle$}{$\langle \sigma; e_0 := e_1 \rangle \rightsquigarrow \langle \sigma'; e_0' := e_1 \rangle$}
    \SequentUnaryNoLabel{$\langle \sigma; e_1 \rangle \rightsquigarrow \langle \sigma'; e_1' \rangle$}{$\langle \sigma; v_0 := e_1 \rangle \rightsquigarrow \langle \sigma'; v_0 := e_1' \rangle$}
    
    \SequentAxiomNoLabel{$\langle (\sigma, l:v, \sigma'); l := v' \rangle \rightsquigarrow \langle (\sigma, l:v', \sigma'); \langle \rangle \rangle$}
}

When it comes to typing this language, we need to make sure that we can type the store. We keep this information in an extra variable $\Sigma$ that we pass around our type derivations.

\newpage
The first five typing rules are taken from STLC and $\Sigma$ is just added to the things we keep track of. The last four deal with references, but only the last one actually uses the contents of $\Sigma$.


\SequentBox{
    $\textrm{Store Typings} \; \Sigma ::= \cdot \; | \; \Sigma, l : X$
    
    \SequentUnary{HYP}{$x : X \in \Gamma$}{$\Sigma;\Gamma\vdash x : X$}
    \SequentAxiom{1l}{$\Sigma;\Gamma\vdash\langle\rangle : 1$}
    \SequentAxiom{$\mathbb{N}$l}{$\Sigma;\Gamma\vdash n : \mathbb{N}$}
    \SequentUnary{$\to$l}{$\Sigma;\Gamma, x:X \vdash e:Y$}{$\Sigma;\Gamma \vdash \lambda x : X . e : X \to Y$}
    \SequentBinary{$\to$E}{$\Sigma;\Gamma \vdash e : X \to Y$}{$\Sigma;\Gamma \vdash e' : X$}{$\Sigma;\Gamma \vdash e e' : Y$}
    \SequentUnary{RefL}{$\Sigma ; \Gamma \vdash e : X$}{$\Sigma ; \Gamma \vdash \textrm{new} \; e : \textrm{ref }X$}
    \SequentUnary{RefGet}{$\Sigma ; \Gamma \vdash e : \textrm{ref }X$}{$\Sigma ; \Gamma \vdash !e : X$}
    \SequentBinary{RefSet}{$\Sigma ; \Gamma \vdash e : \textrm{ref} \; X$}{$\Sigma ; \Gamma \vdash e' : X$}{$\Sigma ; \Gamma \vdash e := e' : 1$}
    \SequentUnary{RefBar}{$l:X \in \Sigma$}{$\Sigma ; \Gamma \vdash l : \textrm{ref} \; X$}
}

We now would want to talk about this language having type safety, but to do this we need to be able to describe that the store at any point of execution is well typed. We're not able to show that an expression is type safe if it is possible for the expression to load something that isn't a valid type from the store.

We define that a store is well typed recursively:

\SequentBox{
    \SequentAxiom{StoreNil}{$\Sigma \vdash \cdot : \cdot$}
    \SequentBinary{StoreCons}{$\Sigma \vdash \sigma' : \Sigma'$}{$\Sigma ; \cdot \vdash v : X$}{$\Sigma \vdash (\sigma', l:v) : (\Sigma', l : X)$}
    \SequentBinary{ConfigOK}{$\Sigma \vdash \sigma : \Sigma$}{$\Sigma ; \cdot \vdash e : X$}{$\langle \sigma ; e \rangle : \langle \Sigma ; X \rangle$}
}

\newpage
We can interpret this in the following way: 
\begin{itemize}
    \item (\textbf{Store Nil})
    
    An empty store is well typed 
    \item (\textbf{Store Cons})
    
    A store can be grown by a single location $l$ and variable $v$ with type $X$ if it can be shown that $v$ has type $X$
    \item (\textbf{Config OK})
    A store is valid for a store configuration if every location in the store matches with the store configuration 
\end{itemize}

Unfortunately if we were to now try to prove type safety naively we'd find ourselves stuck when we try to do structural induction on \textit{new}. This is because the store grows, and so the typing for the store is no longer valid. This leads us to the idea of \textbf{Store Monotonicity}, or that The Store Only Grows.

We define $\Sigma \leq \Sigma'$ to mean there is some other $\Sigma''$ such that $\Sigma' = \Sigma, \Sigma''$. Note that this $\Sigma''$ may just be $\cdot$, hence the less than \textit{or equal to} relation.

We can show that if the store grows then the typing of a store and an expression are still valid. That is,
\begin{equation*}
\begin{split}
    \Sigma ; \Gamma \vdash e : X &\implies \Sigma' ; \Gamma \vdash e : X\\
    \Sigma \vdash \sigma_0 : \Sigma_0 &\implies \Sigma' \vdash \sigma_0 : \Sigma_0
\end{split}
\end{equation*}
With these rules we can define progress and preservation:

\begin{enumerate}
\item  (\textbf{Progress})

Well-typed programs and stores are not stuck: they can always take a step of progress (or are done).

$\langle \sigma; e\rangle : \langle \Sigma; X\rangle \implies (e \in v) \lor (\exists \; \sigma', e'. \langle \sigma; e\rangle \rightsquigarrow \langle \sigma'; e'\rangle)$

\item  (\textbf{Preservation})

If a well-typed program and store take a step, the program will stay well-typed and the store will only grow.

$(\langle \sigma ; e \rangle : \langle \Sigma ; X \rangle) \land (\langle \sigma ; e \rangle \rightsquigarrow \langle \sigma' ; e' \rangle) \implies \exists \; \Sigma' \geq \Sigma.\langle \sigma' ; e' \rangle : \langle \Sigma';X'\rangle$
\end{enumerate}

\newpage
\section{Accidental Looping}

Using our store, we can store references to functions. This is an issue, because functions can themselves read those references when they are run. This lets us create recursion by creating a function that:
\begin{enumerate}
    \item Creates a function that loads a reference and calls the reference
    \item Stores the function in a memory location
    \item Runs the function with the memory location
\end{enumerate}

This can be illustrated with Landin's Knot:

\begin{lstlisting}
let knot : ((int -> int) -> int -> int) -> int -> int =
    fun f ->
        let r = ref (fun n -> 0) in
        let recur = fun n -> !r n in
        let () = r := fun n -> f recur n in recur
\end{lstlisting}

This means that we can build programs that loop forever and so our type system does not result in \textbf{Termination}. This is a shame and it would be nice if we could separate our effectful code from our pure code in a way that prevents infinite loops...

\newpage
\section{Monads}
To stop infinite loops, we need to prevent the formation of structures like Landin's Knot. Landin's Knot was able to be formed because we could both dereference a value and apply that value as a function to an argument within the same program. 

Monads introduce two separate types of instructions and two separate environments in which these instructions execute, called Pure and Impure.

Pure terms operate like traditional lambda calculus - we have values, functions and function compositions. The only thing that we add is that impure terms can be passed around like values. We can't interrogate or execute these impure terms within a pure environment, instead they are opaque and remain unevaluated. 

Impure terms \textit{have effects}. They may read from or write to a store, perform IO, read and write from a terminal. We may also execute Pure terms.

This means that Pure terms may not execute their containing Impure terms, but Impure terms may execute their Pure terms.

One interesting result of this is that a program that has a Pure term as its top level structure must not have any effects, since the Pure term cannot initiate execution of an Impure term.

An initial grammar for a monadic language might be as follows:
\begin{equation*}
\begin{split}
    \textrm{Pure Terms} \; e &::= \langle \rangle \; | \; n \; | \; \lambda x : X . e \; | \; e e' \; | \; l \; | \; {t}\\
    \textrm{Impure Terms} \; t &::= \textrm{new} \; e \; | \; !e \; | \; e := e' \; | \; \textrm{let} \; x = e ; t \; | \; \textrm{return }e\\
    \textrm{Impure Terms} \; e &::= \langle \rangle \; | \; n \; | \; \lambda x : X . e \; | \; l \; | \; {t}\\
    \textrm{Values} \; v &::= \langle \rangle \; | \; n \; | \; \lambda x : X. e \; | \; l \; | \; \{t\}\\
    \textrm{Stores} \; \sigma &::= \cdot \; | \; \sigma, l : v\\
    \textrm{Contexts} \; \Gamma &::= \cdot \; | \; \Gamma, x : X\\
    \textrm{Store Typings} \; \Sigma &::= \cdot \; | \; \Sigma, l : X
\end{split}
\end{equation*}

The differences to notice are:
\begin{enumerate}
    \item We have two new types, ref $X$ and $TX$. The first describes the type of a store location, and the second describes the type as the result of an Impure expression, or Monad. For example, $3$ has the type $\mathbb{N}$ while $\{return 3\}$ has the type $T\mathbb{N}$
    \item We have two new Pure terms, $l$ and $\{t\}$. The first represents a location which should be generated by new $e$ under normal circumstances, but should be able to be typechecked so that we can show \textbf{Type Safety} later on. The second is how we embed Impure expressions within pure ones, by wrapping the impure statement in curly braces. This links into one of our new value types, as Impure expressions wrapped in curly braces are values and so when encountered in a Pure context are not reduced at all.
    \item We have a new category called Impure types which consist of everything that can manipulate the store, as well as the \textbf{let} and \textbf{return} expressions. 
    
    We can see the let expression as temporarily exposing the content of a monad. That is, the let expression is a bit like a function of the type $T\alpha \to (\alpha \to T \beta) \to T \beta$, except its second argument isn't given as a function but instead an expression with a free variable.
    
    The \textbf{return} expression is the counterpart to let and allows us to create a new monad, that exposes whatever is returned to the let expression. 
    
    One way of viewing these two commands (and monads in general) is as a many-layered object like a burrito or an onion. When an object is wrapped in curly brackets we wrap the monad with an extra layer, obscuring the mess inside. This wrapped object is itself inert and cannot be queried or evaluated. However a \textbf{let} expression allows us to peel back one layer of the monad and access the value inside, so long as the expression that uses the value wraps the result back up at the end. 
\end{enumerate}

Since it can be quite hard to read this grammar, here are some examples of programs:
$$(\lambda x: T1 . x)\{\textrm{let} \; x = 1; x := 2\}$$
This first equation reduces to $\{\textrm{let} \; x = 1; x := 2\}$ and then halts. This is because an Impure term wrapped in curly braces is a value, said to be a \textbf{Suspended Computation}, and is not itself evaluated. This means that any expression for which the 'top level' component is Pure must have no side effects.
$$\textrm{let} \; y = \lambda x: 1. x; \textrm{let} \; z = \textrm{new} \; y$$
This second expression displays that we can still store functions in the store. We could even load this store and run the function. The difference is that this function itself cannot load from the store, since functions must be Pure. This is how we avoid the formation of Landin's Knot.
$$\textrm{let} \; x = (\textrm{if True then }\{ \textrm{return} \; 1\}\; \textrm{else }\{ \textrm{return} \; 2\}); return !x$$
This program illustrates the nesting of Pure and Impure programs. The outer layer is Impure with a \textbf{let} statement, then within this is a Pure \textbf{lambda} expression, which contains two impure \textbf{return} statements.

\newpage

More formally, here are the typing rules. Note the two different forms of typing derivations, using $\Sigma ; \Gamma \vdash e : \tau$ for Pure expressions and $\Sigma ; \Gamma \vdash e \div \tau$ for Impure expressions.

Also notice that HYP, 1l, $\mathbb{N}$l, $\to$l and $\to$E are exactly the same as STLC just with a $\Sigma$ passed through.

\SequentBox{
    \SequentUnary{HYP}{$x : X \in \Gamma$}{$\Sigma;\Gamma\vdash x : X$}
    \SequentAxiom{1l}{$\Sigma;\Gamma\vdash\langle\rangle : 1$}
    \SequentAxiom{$\mathbb{N}$l}{$\Sigma;\Gamma\vdash n : \mathbb{N}$}
    \SequentUnary{$\to$l}{$\Sigma;\Gamma, x:X \vdash e:Y$}{$\Sigma;\Gamma \vdash \lambda x : X . e : X \to Y$}
    \SequentBinary{$\to$E}{$\Sigma;\Gamma \vdash e : X \to Y$}{$\Sigma;\Gamma \vdash e' : X$}{$\Sigma;\Gamma \vdash e e' : Y$}
    \SequentUnary{RefBar}{$l:X \in \Sigma$}{$\Sigma ; \Gamma \vdash l : \textrm{ref }X$}
    \SequentUnary{Tl}{$\Sigma ; \Gamma \vdash t \div X$}{$\Sigma ; \Gamma \vdash \{ t \} : TX$}
    
    
    \SequentUnary{RefL}{$\Sigma ; \Gamma \vdash e : X$}{$\Sigma ; \Gamma \vdash \textrm{new} \; e \div \textrm{ref }X$}
    \SequentUnary{RefGet}{$\Sigma ; \Gamma \vdash e : \textrm{ref }X$}{$\Sigma ; \Gamma \vdash !e \div X$}
    \SequentBinary{RefSet}{$\Sigma ; \Gamma \vdash e : \textrm{ref} \; X$}{$\Sigma ; \Gamma \vdash e' : X$}{$\Sigma ; \Gamma \vdash e := e' \div 1$}
    \SequentUnary{TRet}{$\Sigma ; \Gamma \vdash e : X$}{$\Sigma ; \Gamma \vdash \textrm{return} \; e \div X$}
    \SequentBinary{TLet}{$\Sigma ; \Gamma \vdash e : TX$}{$\Sigma ; \Gamma, x:X \vdash t \div Z$}{$\Sigma ; \Gamma \vdash \textrm{let} \; x = e ; t \div Z$}
}

\newpage
We then define our operational semantics on two levels. We have our very basic function application semantics for Pure expressions, and then our operational semantics for Impure expressions which caries with it a store. Note that we could extend our monad system however we wanted to include any other kinds of effects, and these would all be tracked inside this impure semantics.

\SequentBox{
    \SequentUnaryNoLabel{$e_0 \rightsquigarrow e_0'$}{$e_0 e_1 \rightsquigarrow e_0' e_1$}
    \SequentUnaryNoLabel{$e_1 \rightsquigarrow e_1'$}{$v_0 e_1 \rightsquigarrow v_0 e_1'$}
    
    \SequentAxiomNoLabel{$(\lambda x : X . e) v \rightsquigarrow [v / x] e$}
    
    \SequentUnaryNoLabel{$e \rightsquigarrow e'$}{$\langle \sigma ; \textrm{new} \; e \rangle \rightsquigarrow \langle \sigma ; \textrm{new} \; e' \rangle$}
    \SequentUnaryNoLabel{$l \notin dom(\sigma)$}{$\langle \sigma ; \textrm{new} \; e \rangle \rightsquigarrow \langle (\sigma, l:v);\textrm{return} \; l \rangle$}
    \SequentUnaryNoLabel{$e \rightsquigarrow e'$}{$\langle \sigma ; ! e \rangle \rightsquigarrow \langle \sigma ; ! e' \rangle$}
    \SequentUnaryNoLabel{$l : v \in \sigma$}{$\langle \sigma ; ! l \rangle \rightsquigarrow \langle (\sigma, l:v);\textrm{return} \; v \rangle$}
    \SequentUnaryNoLabel{$e_0 \rightsquigarrow e_0'$}{$\langle \sigma ; e_0 := e_1 \rangle \rightsquigarrow \langle \sigma ; e_0' := e_1 \rangle$}
    \SequentUnaryNoLabel{$e_1 \rightsquigarrow e_1'$}{$\langle \sigma ; v_0 := e_1 \rangle \rightsquigarrow \langle \sigma ; v_0 := e_1' \rangle$}
    \SequentAxiomNoLabel{$\langle ( \sigma, l:v, \sigma');l:=v'\rangle \rightsquigarrow \langle (\sigma, l:v', \sigma');\textrm{return} \; \langle \rangle \rangle$}
    
    \SequentUnaryNoLabel{$e \rightsquigarrow e'$}{$\langle \sigma ; \textrm{return} \; e \rangle \rightsquigarrow \langle \sigma ; \textrm{return} \; e' \rangle$}
    \SequentUnaryNoLabel{$e \rightsquigarrow e'$}{$\langle \sigma ; \textrm{let} \; x = e ; t \rangle \rightsquigarrow \langle \sigma ; \textrm{let} \; x = e' ; t \rangle$}
    
    \SequentAxiomNoLabel{$\langle \sigma ; \textrm{let }x = \{\textrm{return} \; v\} ; t_1\rangle \rightsquigarrow \langle \sigma ; [v/x]t_1\rangle$}
    \SequentUnaryNoLabel{$\langle \sigma ; t_0 \rangle \rightsquigarrow \langle \sigma' ; t_0' \rangle$}{$\langle \sigma ; \textrm{let }x = \{ t_0 \} ; t_1 \rangle \rightsquigarrow \langle \sigma' ; \textrm{let} \; x = \{t_0'\} ; t_1 \rangle$}
}

Things that are worth noting here: 
\begin{itemize}
    \item Since all effectful code is strictly linear, and the only way to deviate from the chosen path of execution is function application which is done on the Pure level, we have preserved our halting properties from before.
    \item Also, since all effectful code is strictly linear, the only rule that uses progress of Impure code is the final \textbf{let} rule. This rule propagates the execution step into the argument to be assigned to the variable until the argument is a value. Every other Impure rule either steps a containing Pure argument (and so doesn't change the store), or does some work on itself, possibly changing the store. This demonstrates the seperation of Pure and Impure code in that any containing Pure code is not handed the store and cannot change the store, so must be independent of the result of any Impure code. 
    \item As seen in the lectures, we can see that the Operational Semantics for Pure statements are the first three rules and no more. If we add other effects such as \textbf{print} or \textbf{read} then we may need to change the store that is threaded around the Impure rules, but the Pure part of this system always stays the same.
\end{itemize}

We can prove \textbf{Weakening}, \textbf{Exchange} and \textbf{Substitution} for the set $\Gamma$ for both Pure and Impure types. Since the proof of each property on Pure expressions would require the property to be true on Impure types and the other way around too, prove each property on both Pure and Impure at the same time (mutually inductively) so as to be able to use structural induction using every rule for every possible expression form. This way you can use the inductive hypothesis on both Pure and Impure expressions. \textbf{Progress} and \textbf{Preservation} can be proven mutually inductively in a similar way, giving us \textbf{Type Safety}.

And as we discussed earlier, we can see that Pure expressions cannot depend on the results of Impure expressions, so Pure expressions must still have the property of \textbf{Termination} that we showed for STLC.

\centerboxtitled{Extra Information:}{
    Often, this \textbf{let} expression as described above is instead called \textbf{bind}, and has exactly the signature that we described before:
    $$bind: m\alpha \to (\alpha \to m\beta) \to m\beta$$
    We also have the \textbf{return} function, which has the type
    $$return: \alpha \to m\alpha$$
    However there is a more simple function, called \textbf{join}, which simply takes to monadic 'black boxes' and combines them together:
    $$join: m(m\alpha) \to m\alpha$$
    We can represent this join function in our language using two let expressions:
    $$\textrm{let} \; x = v; \textrm{let} \; y = x; \textrm{return} \; y$$
}

\newpage
\section{Classical Logic}

We saw with the Curry Howard correspondence that Simply Typed lambda calculus can be used to create instances of types that prove corresponding First Order Intuitionistic Propositional statements. We then saw that we could further introduce Universal and Existential quantifiers through System F to gain more proof power, allowing us to create instances of types that prove corresponding \textit{Second Order} Intuitionistic Propositional statements.

However we are still one Double Negation Elimination axiom away from being able to prove any true statement within Classical logic.

The issue is that within Intuitionistic Logic we have a slightly different notion of Falsehood. We write $\lnot P$ to mean that $P \implies \bot$, or that $T_P \to \bot$. In other words, if we are able to give an instance of type $T_P$, then we are also able to give an instance of type $\bot$, however since we cannot ever have a type of $\bot$ we must not be able to give an instance of $T_P$.

This means that, within Intuitionistic Logic, we have ideas of \textit{Proven} and \textit{Unprovable}, rather than Classical Logic's \textit{True} and \textit{False}.

We could therefore begin to build up to Classical Logic by treating Refutations, or Proofs of Unprovability, as a first-class notion of Falsehood. This moves away from STLC where the only first-class object is a type and instances of a type represent proofs of a true proposition. By having two different first-class objects, \textit{true} and \textit{false}, we find that an instance of a type is now either a proof that a proposition is true or that it is false.

To expand on this, imagine first we want to prove that some proposition $P$ holds. Within STLC, it is sufficient to show that some expression with type $T_P$ exists. However within our new system, we will want to show that some expression with type $T_P \; true$ exists. This allows us to prove that a proposition $P$ is false by showing that some expression in our system has the type $T_P \; false$.

We now must establish what exactly these expressions with these types are. We know that our types now have an extra tag stating \textit{true} or \textit{false}, but we need to devise a type of expression which has these types.

The trick is in going backwards; we know that we must be able to use our expressions as proofs of the proposition that their type corresponds to, so we can let our expressions be the encoding of proofs of statements within Classical Logic.

That is, if we can establish firstly a system for proving all true or false statements in Classical Logic, and then secondly a way to encode these proofs as expressions, then each expression can have the type that corresponds to the proven true or false proposition.

\newpage
We know from IB Logic and Proof that we can use sequents to prove propositions within Classical Logic:

\SequentBox{
    \begin{spacing}{1.0}
        \inlineeq{
        \begin{split}
        \textrm{Propositions} \; A &::= \top \; | \; A \land B \; | \; \bot \; | \; A \lor B \; | \; \lnot A\\
        \textrm{True contexts} \; \Gamma &::= \cdot \; | \; \Gamma, A\\
        \textrm{False contexts} \; \Delta &::= \cdot \; | \; \Delta, A\\
        \textrm{Typing Judgements} \; &::= \Gamma; \Delta \vdash A \; \textrm{true} \; | \; \Gamma; \Delta \vdash A \; \textrm{false} \; | \; \Gamma; \Delta \vdash \textrm{contr} 
        \end{split}
        }
    \end{spacing}
    
    \vspace{30px}
    
    \SequentUnary{HypP}{$A \in \Gamma$}{$\Gamma ; \Delta \vdash A$ true}
    \SequentUnary{HypR}{$A \in \Delta$}{$\Gamma ; \Delta \vdash A$ false}
    
    \SequentAxiom{$\top$P}{$\Gamma ; \Delta \vdash \top$ true}
    \SequentAxiom{$\bot$R}{$\Gamma ; \Delta \vdash \bot$ false}
    
    \SequentBinary{$\land$P}{$\Gamma ; \Delta \vdash A$ true}{$\Gamma ; \Delta \vdash B$ true}{$\Gamma ; \Delta \vdash A \land B$ true}
    \SequentBinary{$\lor$R}{$\Gamma ; \Delta \vdash A$ false}{$\Gamma ; \Delta \vdash B$ false}{$\Gamma ; \Delta \vdash A \lor B$ false}
    
    \SequentUnary{$\lor$P$_1$}{$\Gamma ; \Delta \vdash A$ true}{$\Gamma ; \Delta \vdash A \lor B$ true}
    \SequentUnary{$\lor$P$_2$}{$\Gamma ; \Delta \vdash B$ true}{$\Gamma ; \Delta \vdash A \lor B$ true}
    \SequentUnary{$\land$R$_1$}{$\Gamma ; \Delta \vdash A$ false}{$\Gamma ; \Delta \vdash A \land B$ false}
    \SequentUnary{$\land$R$_2$}{$\Gamma ; \Delta \vdash B$ false}{$\Gamma ; \Delta \vdash A \land B$ false}
    
    \SequentUnary{$\lnot$P}{$\Gamma ; \Delta \vdash A$ false}{$\Gamma ; \Delta \vdash \lnot A$ true}
    \SequentUnary{$\lnot$R}{$\Gamma ; \Delta \vdash A$ true}{$\Gamma ; \Delta \vdash \lnot A$ false}
    
    \SequentUnaryNoLabel{$\Gamma ; \Delta, A \vdash $contr}{$\Gamma ; \Delta \vdash A$ true}
    \SequentUnaryNoLabel{$\Gamma, A ; \Delta \vdash $contr}{$\Gamma ; \Delta \vdash A$ false}
    
    \SequentBinary{Contr}{$\Gamma ; \Delta \vdash A$ true}{$\Gamma ; \Delta \vdash A$ false}{$\Gamma ; \Delta \vdash $contr}
}

\newpage
With these rules we've gained the ability to prove the Double Negation Elimination rule: 

\begin{center}
\AxiomC{}
\SafeRightLabel{HYP}
\UnaryInfC{$A ; \cdot \vdash A$ true}
\SafeRightLabel{$\lnot$R}
\UnaryInfC{$A ; \cdot \vdash \lnot A$ false}
\SafeRightLabel{$\lnot$P}
\UnaryInfC{$A ; \cdot \vdash \lnot \lnot A$ true}
\DisplayProof
\hspace{3em}
\AxiomC{}
\UnaryInfC{$\lnot \lnot A ; A \vdash \lnot \lnot A$ true}
\AxiomC{}
\UnaryInfC{$\lnot \lnot A ; A \vdash A$ false}
\UnaryInfC{$\lnot \lnot A ; A \vdash \lnot A$ true}
\UnaryInfC{$\lnot \lnot A ; A \vdash \lnot \lnot A$ false}
\BinaryInfC{$\lnot \lnot A ; A \vdash$ contr}
\UnaryInfC{$\lnot \lnot A ; \cdot \vdash A$ true}
\DisplayProof
\end{center}

We can now devise a way to encode these rules as values within our new language. We have a parity between the rules, where every proposition form has exactly two sequents that it corresponds to; one for if the proposition is true and one for if it was false. We must therefore include in our encoding whether an expression is true or false, which we can do by introducing an alternative variation of each value, dubbed a Continuation.

\newpage
This gives us the following grammar:

\inlineeq{
\begin{split}
\textrm{Values} \; e &::= \langle\rangle \; | \; \langle e, e' \rangle \; | \; L e \; | \; R e \; | \; \textrm{not}(k) \; | \; \mu u : A. c\\
\textrm{Continuations} \; k &::= [] \; | \; [k, k'] \; | \; \textrm{fst} \; k \; | \; \textrm{snd} \; k \; | \; \textrm{not}(e) \; | \; \mu x : A. c\\
\textrm{Contradictions} \; c &::= \langle e |_A k\rangle
\end{split}
}

Note the correspondence between Values and Continuations.

We can then type these expressions with the types that we established earlier:

\SequentBox{
    \SequentUnary{HypP}{$x: A \in \Gamma$}{$\Gamma ; \Delta \vdash x : A$ true}
    \SequentUnary{HypR}{$x: A \in \Delta$}{$\Gamma ; \Delta \vdash x : A$ false}
    
    \SequentAxiom{$\top$P}{$\Gamma ; \Delta \vdash \langle \rangle : \top$ true}
    \SequentAxiom{$\bot$R}{$\Gamma ; \Delta \vdash [] : \bot$ false}
    
    \SequentBinary{$\land$P}{$\Gamma ; \Delta \vdash e: A$ true}{$\Gamma ; \Delta \vdash e': B$ true}{$\Gamma ; \Delta \vdash \langle e, e' \rangle A \land B$ true}
    \SequentBinary{$\lor$R}{$\Gamma ; \Delta \vdash k : A$ false}{$\Gamma ; \Delta \vdash k' : B$ false}{$\Gamma ; \Delta \vdash [k, k'] : A \lor B$ false}
    
    \SequentUnary{$\lor$P$_1$}{$\Gamma ; \Delta \vdash e: A$ true}{$\Gamma ; \Delta \vdash \textrm{L}e : A \lor B$ true}
    \SequentUnary{$\lor$P$_2$}{$\Gamma ; \Delta \vdash e: B$ true}{$\Gamma ; \Delta \vdash \textrm{R}e : A \lor B$ true}
    \SequentUnary{$\land$R$_1$}{$\Gamma ; \Delta \vdash k: A$ false}{$\Gamma ; \Delta \vdash \textrm{fst} \; k : A \land B$ false}
    \SequentUnary{$\land$R$_2$}{$\Gamma ; \Delta \vdash k: B$ false}{$\Gamma ; \Delta \vdash \textrm{snd} \; k : A \land B$ false}
    
    \SequentUnary{$\lnot$P}{$\Gamma ; \Delta \vdash k: A$ false}{$\Gamma ; \Delta \vdash \textrm{not}(k) : \lnot A$ true}
    \SequentUnary{$\lnot$R}{$\Gamma ; \Delta \vdash e: A$ true}{$\Gamma ; \Delta \vdash \textrm{not}(e) : \lnot A$ false}
    
    \SequentUnaryNoLabel{$\Gamma ; \Delta, u: A \vdash c$ contr}{$\Gamma ; \Delta \vdash \mu u : A.c: A$ true}
    \SequentUnaryNoLabel{$\Gamma, x: A ; \Delta \vdash c$ contr}{$\Gamma ; \Delta \vdash \mu x : A.c: A$ false}
    
    \SequentBinary{Contr}{$\Gamma ; \Delta \vdash e: A$ true}{$\Gamma ; \Delta \vdash k: A$ false}{$\Gamma ; \Delta \vdash \langle e |_A k \rangle$ contr}
}

Note that the set of things that we have assumed to be true $\Gamma$ and the set of things we have assumed to be false $\Delta$ now become our variable contexts. This gives added meaning to an expression having no free variables - it must correspond to a tautology. 

\newpage

As with the Curry Howard Correspondence, we can imagine that evaluation of an expression within this new language would be akin to normalising the proof for it. Since most proofs within this language will take the form of a contradiction, we can observe a few things about proving with contradictions:
\begin{itemize}
    \item If a contradiction exists with the type $A \land B$, and the evidence that $A \land B$ is false is of the form fst $k$, then there must actually be a contradiction specifically with $A$.
    \item If a contradiction exists with the type $\lnot A$ and both evidence are of the form not$(e)$ and not$(k)$, then a contradiction must exist with $A$ with the evidence being $e$ and $k$.
    \item If a contradiction exists with the type $A$, and the evidence $\mu u: A.c$ that $A$ is true directly uses a contradiction, then we can simply use the evidence of the contradiction as the contradiction.
\end{itemize}

To see that last point more clearly, consider the following two proof trees:
\begin{center}
\AxiomC{$\Gamma ; \Delta, A \vdash B$ true}
\AxiomC{$\Gamma ; \Delta, A \vdash B$ false}
\BinaryInfC{$\Gamma ; \Delta, A \vdash$ contr}
\UnaryInfC{$\Gamma ; \Delta \vdash A$ true}
\AxiomC{$\Gamma ; \Delta \vdash A$ false}
\BinaryInfC{$\Gamma ; \Delta \vdash$ contr}
\DisplayProof

\vspace{1em}

\AxiomC{$\Gamma ; \Delta \vdash B$ true}
\AxiomC{$\Gamma ; \Delta \vdash B$ false}
\BinaryInfC{$\Gamma ; \Delta \vdash$ contr}
\DisplayProof
\end{center}

Both show the same thing, which is that $\Gamma ; \Delta$ leads to a contradiction. We can transform the first into the second since we can take the leftmost branch of the first tree and substitute our proof that A is false in the rightmost branch.

This gives us the following operational semantics:

\SequentBox{
\begin{spacing}{1.0}
    \inlineeq{
    \begin{split}
    \langle \langle e_1, e_2\rangle  \; |_{A \land B} \; \textrm{fst } k\rangle &\rightsquigarrow  \langle e_1 \; |_A k\rangle \\
    \langle \langle e_1, e_2\rangle  \; |_{A \land B} \; \textrm{snd } k\rangle &\rightsquigarrow  \langle e_2 \; |_B \; k\rangle \\
    \langle L e \; |_{A \lor B} \; [k_1, k_2]\rangle &\rightsquigarrow \langle e \; |_A \; k_1\rangle \\
    \langle R e \; |_{A \lor B} \; [k_1, k_2]\rangle &\rightsquigarrow \langle e \; |_B \; k_2\rangle \\
    \langle \textrm{not}(k) \; |_{\lnot A} \; \textrm{not}(e)\rangle &\rightsquigarrow \langle e \; |_A \; k\rangle \\
    \langle \mu u : A. c \; |_A \; k\rangle &\rightsquigarrow  [k/u]c\\
    \langle e \; |_A \; \mu x : A. c\rangle &\rightsquigarrow  [e/x]c\\
    \end{split}
    }
\end{spacing}
\vspace{-10px}
}

\newpage
\section{The Computational Ability of Classical Logic}
When inventing our new Classical Logic computation system, we've removed the lambda expressions. Luckily, if you squint hard enough, our new contradiction terms look a bit like lambda expressions. In fact, we have the operational semantics $\langle \mu u : A. c \; |_A \; k\rangle \rightsquigarrow  [k/u]c$, which looks to me like $(\lambda u: A. c) k \rightsquigarrow  [k/u]c$. 

\textbf{Progress} can be written that if $\cdot ; \cdot \vdash c$ contr then $c \rightsquigarrow c'$ (or $c$ final). Unfortunately, if Classical Logic is consistent (which it is), we know that there do not exist any such expressions that are both closed and contradictions. Therefore our language is functionally useless.

We can try to fix this if we want by introducing extra values, called \textit{halt}, \textit{ans} and \textit{done} to be able to form contradictions where there are none and so allow for arbitrary computations. Unfortunately, by doing this we completely break all of the consistency of Classical Logic; if any contradiction can be formed at any time, we can prove that anything holds.

This appears to give us a choice: we can either use our Classical Logic in a consistent state, or in a computationally useful way. We will see that there is a different approach that we can take towards this whole thing that allows us to have both.

\newpage
\section{Classical Embedding}

We introduce the concept of Embedding Classical logic with an observation: 

\inlineeq{\lambda k : ((X \to \bot) \to \bot) \to \bot . \lambda a: X. k (\lambda q: X \to \bot. q a))\\
\textrm{has the type}\\
((X \to \bot) \to \bot) \to \bot \to (X \to \bot)}

If we use our previous observation that we can equate the proposition $\lnot A$ to the type $A \to \bot$, we have proven that:

\inlineeq{\lnot\lnot\lnot X \implies \lnot X}

This is called triple negation, and holds within Intuitionistic logic.

\centerboxtitled{Small Aside}{Why is it then that Double Negation Elimination doesn't hold but Triple Negation Elimination does?

The trick is in the Intuitionistic interpretation of False as 'Not Able to be Proven'.

Imagine that you and a group of researchers find some ancient cave writings. These writings might be written by aliens, making the statements written True, or they might be scribbles written by children, making them Nonsensical and Unable to be proven True. The children's scribbles may be completely True statements about the meaning of Life, the Universe and Everything, however unfortunately the children's handwriting is not good enough for us to ever decode these statements.

Now imagine that I told you that the writing is \textit{Not} the children's. This means that the statements contained must be True, and follows the classical idea of Double Negation Elimination. 

Imagine instead that I told you that we are \textit{Unable to Prove} that the writing is the Children's. Then we do not know anything more about the validity of the contained statements, and so Double Negation Elimination does not hold.

Now imagine that I told you that we would \textit{Never be Able to Prove} whether or not we are \textit{Unable to Prove} that the writing is the Children's. Then we will also \textit{Never be Able to Prove} that we are \textit{\textbf{Able} to Prove} that the writing is the Alien's, since showing that the writing is the Alien's proves that we are unable to show that the writing is the Children's. Therefore we will never know whether the contained statements are True or not, which means that we are functionally in the same place as if we knew that the writing was the Children's. This extra layer of Unprovability allows us to essentially use the Law of Excluded Middle, that $\lnot \lnot \lnot A \lor \lnot \lnot A$ holds.
}

\newpage

Note that we can define a far more relaxed version of negation where Triple Negation Elimination still holds, called \textbf{quasi-negation}.

We take any proposition p and define quasi-negation $\quasi X$ as $X \to T_p$. This type equates to the proposition $X \implies P$ for any P. This logically makes sense since we know that $\bot \implies$ \textit{anything} by the Principle of Explosion ("\textit{ex falso quodlibet}").

Using this quasi-negation, we define a translation for Classical Statements into STLC as follows:

\inlineeq{
\begin{split}
    (\lnot A)^{\circ} &= \quasi A^{\circ}\\
    \top^{\circ} &= 1\\
    (A \land B)^{\circ} &= A^{\circ} \times B^{\circ}\\
    \bot^{\circ} &= p\\
    (A \land B)^{\circ} &= \quasi\quasi (A^{\circ} + B^{\circ})
\end{split}
}

Note the extra double-negation on the union term. This is only one possible encoding of Classical Logic into a form for which Double Negation Elimination holds. For example, we could place a double negation in front of every term (known as the Kolmogorov Translation), or the de Morgan duality for disjunction $\quasi (\quasi A^{\circ} \times \quasi B^{\circ})$ to avoid unions all together.

We can prove that double negation elimination holds for all of these encoded terms within Intuitionistic logic by creating functions that have the following types:

\inlineeq{
\begin{split}
    \cdot \vdash \textrm{dne}_A&: \quasi \quasi A^{\circ} \to A^{\circ}\\
    &\\
    \cdot \vdash \textrm{dne}_\top &: \quasi \quasi 1 \to 1 \\
    \textrm{dne}_\top &= \lambda q. \langle \rangle \\
    &\\
    \cdot \vdash \textrm{dne}_{A \land B}&: \quasi \quasi A^{\circ} \times B^{\circ} \to A^{\circ} \times B^{\circ}\\
    \textrm{dne}_{A \land B} &= \lambda q. \langle \substack{\textrm{dne}_A (\lambda k. q(\lambda p. k(\textrm{fst } p)))\\\textrm{dne}_B (\lambda k. q(\lambda p. k(\textrm{snd } p)))} \rangle\\
    &\\
    \cdot \vdash \textrm{dne}_\bot &: \quasi \quasi p \to p \\
    \textrm{dne}_\bot &= \lambda q. q (\lambda x. x) \\
    &\\
    \cdot \vdash \textrm{dne}_{A \lor B} &: \quasi \quasi \quasi \quasi (A^{\circ} + B^{\circ}) \to \quasi \quasi (A^{\circ} + B^{\circ}) \\
    \textrm{dne}_{A \lor B} &= \lambda q. (\lambda k. \lambda a. k (\lambda q. q a)) q \\
    &\\
    \cdot \vdash \textrm{dne}_{\lnot A} &: \quasi \quasi (\quasi A^{\circ}) \to \quasi A^{\circ} \\
    \textrm{dne}_{\lnot A} &= \lambda q. (\lambda k. \lambda a. k (\lambda q. q a)) q 
\end{split}
}

Note the similarity between the terms for dne$_{A \lor B}$ and dne$_\lnot A$, and the Triple Negation Elimination term $\lambda k. \lambda a. k (\lambda q. q a)$. We call this term \textit{tne} for short.

\newpage

Now that we have established an embedded form of Intuituinistic logic for which Double Negation Elimination holds, we can begin mapping our encoding of Classical logic into this embedding.



\end{document}